{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "MAX_SENTENCE_LENGTH = 300\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"i\",\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"to\",\"from\",\"up\",\"down\",\"in\",\"on\",\"off\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"other\",\"some\",\"such\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "tokenize = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_pos = \"/home/cvh255/nlp_hw1/aclImdb/train/pos/\"\n",
    "path_train_neg = \"/home/cvh255/nlp_hw1/aclImdb/train/neg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_files = os.listdir(path_train_pos)\n",
    "for f in range(len(train_pos_files)):\n",
    "    train_pos_files[f] = path_train_pos + train_pos_files[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_files = os.listdir(path_train_neg)\n",
    "for f in range(len(train_neg_files)):\n",
    "    train_neg_files[f] = path_train_neg + train_neg_files[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_labels = [1]*len(train_pos_files)\n",
    "train_neg_labels = [0]*len(train_neg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"file_names\",\"labels\"])\n",
    "df[\"file_names\"] = train_pos_files+train_neg_files\n",
    "df[\"labels\"] = train_pos_labels+train_neg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9258_10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3_10.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9597_10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3347_7.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/2160_8.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          file_names  labels\n",
       "0  /home/cvh255/nlp_hw1/aclImdb/train/pos/9258_10...       1\n",
       "1    /home/cvh255/nlp_hw1/aclImdb/train/pos/3_10.txt       1\n",
       "2  /home/cvh255/nlp_hw1/aclImdb/train/pos/9597_10...       1\n",
       "3  /home/cvh255/nlp_hw1/aclImdb/train/pos/3347_7.txt       1\n",
       "4  /home/cvh255/nlp_hw1/aclImdb/train/pos/2160_8.txt       1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(df[\"file_names\"], df[\"labels\"]):\n",
    "    train_df = df.iloc[train_index]\n",
    "    val_df = df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 2), (5000, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape,val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "    all_txt = []\n",
    "    for i,j in df.iterrows():\n",
    "        f = open(j[\"file_names\"])\n",
    "        txt = f.read()\n",
    "        all_txt.append(txt)\n",
    "#         print(j)\n",
    "    df[\"content\"] = all_txt\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_df = get_data(train_df)\n",
    "val_df = get_data(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation\n",
    "def tokenize1(phrase):\n",
    "    tokens = tokenize(phrase)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations and token.text not in stop_words)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = list(nltk.trigrams(tokenize1(\"I am going mad! you are nuts\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'going', 'mad'), ('going', 'mad', 'nuts')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i going mad', 'going mad nuts']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[' '.join(a) for a in bg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset1(dataset,n_gram):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize1(sample)\n",
    "        bg = list(nltk.bigrams(tokens))\n",
    "        tg = list(nltk.trigrams(tokens))\n",
    "        bg_t = [' '.join(a) for a in bg]\n",
    "        tg_t = [' '.join(a) for a in tg]\n",
    "        tokens = tokens + bg_t + tg_t\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens+=tokens\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing val data\")\n",
    "val_df[\"tokenized1\"], _ = tokenize_dataset1(val_df[\"content\"],1)\n",
    "# pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_df[\"tokenized1\"], all_train_tokens = tokenize_dataset1(train_df[\"content\"],1)\n",
    "# pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>content</th>\n",
       "      <th>tokenized1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8283</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>After seeing this film I feel like I know just...</td>\n",
       "      <td>[after, seeing, film, i, feel, like, i, know, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...</td>\n",
       "      <td>1</td>\n",
       "      <td>My son was 7 years old when he saw this movie,...</td>\n",
       "      <td>[my, son, 7, years, old, saw, movie, russian, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9347</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Remember the early days of Pay Per View? I do,...</td>\n",
       "      <td>[remember, early, days, pay, per, view, i, alm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...</td>\n",
       "      <td>1</td>\n",
       "      <td>And that's how the greatest comedy of TV start...</td>\n",
       "      <td>[and, 's, greatest, comedy, tv, started, it, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Lily Mars, a smalltown girl living in Indiana,...</td>\n",
       "      <td>[lily, mars, smalltown, girl, living, indiana,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_names  labels  \\\n",
       "8283   /home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt       1   \n",
       "10937  /home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...       1   \n",
       "9347   /home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt       1   \n",
       "5430   /home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...       1   \n",
       "4072   /home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt       1   \n",
       "\n",
       "                                                 content  \\\n",
       "8283   After seeing this film I feel like I know just...   \n",
       "10937  My son was 7 years old when he saw this movie,...   \n",
       "9347   Remember the early days of Pay Per View? I do,...   \n",
       "5430   And that's how the greatest comedy of TV start...   \n",
       "4072   Lily Mars, a smalltown girl living in Indiana,...   \n",
       "\n",
       "                                              tokenized1  \n",
       "8283   [after, seeing, film, i, feel, like, i, know, ...  \n",
       "10937  [my, son, 7, years, old, saw, movie, russian, ...  \n",
       "9347   [remember, early, days, pay, per, view, i, alm...  \n",
       "5430   [and, 's, greatest, comedy, tv, started, it, 1...  \n",
       "4072   [lily, mars, smalltown, girl, living, indiana,...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_df_tok4.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.to_csv(\"val_df_tok4.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(all_train_tokens,open(\"all_tokens4\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_df_tok1.csv\")\n",
    "val_df = pd.read_csv(\"val_df_tok1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "all_train_tokens = pickle.load(open(\"all_tokens1\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 100000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 50494 ; token the ten\n",
      "Token the ten; token id 50494\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_df['token_idized'] = token2index_dataset(train_df['tokenized1'])\n",
    "val_df['token_idized'] = token2index_dataset(val_df['tokenized1'])\n",
    "# test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>content</th>\n",
       "      <th>tokenized1</th>\n",
       "      <th>token_idized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8283</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>After seeing this film I feel like I know just...</td>\n",
       "      <td>[after, seeing, film, i, feel, like, i, know, ...</td>\n",
       "      <td>[398, 253, 6, 2, 164, 11, 2, 60, 52, 154, 3831...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...</td>\n",
       "      <td>1</td>\n",
       "      <td>My son was 7 years old when he saw this movie,...</td>\n",
       "      <td>[my, son, 7, years, old, saw, movie, russian, ...</td>\n",
       "      <td>[303, 407, 1309, 86, 83, 140, 5, 1959, 7328, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9347</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Remember the early days of Pay Per View? I do,...</td>\n",
       "      <td>[remember, early, days, pay, per, view, i, alm...</td>\n",
       "      <td>[320, 334, 456, 997, 4080, 642, 2, 147, 320, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...</td>\n",
       "      <td>1</td>\n",
       "      <td>And that's how the greatest comedy of TV start...</td>\n",
       "      <td>[and, 's, greatest, comedy, tv, started, it, 1...</td>\n",
       "      <td>[57, 3, 827, 143, 173, 613, 12, 2462, 86, 170,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Lily Mars, a smalltown girl living in Indiana,...</td>\n",
       "      <td>[lily, mars, smalltown, girl, living, indiana,...</td>\n",
       "      <td>[4333, 6196, 1, 168, 555, 9950, 1466, 160, 127...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_names  labels  \\\n",
       "8283   /home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt       1   \n",
       "10937  /home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...       1   \n",
       "9347   /home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt       1   \n",
       "5430   /home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...       1   \n",
       "4072   /home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt       1   \n",
       "\n",
       "                                                 content  \\\n",
       "8283   After seeing this film I feel like I know just...   \n",
       "10937  My son was 7 years old when he saw this movie,...   \n",
       "9347   Remember the early days of Pay Per View? I do,...   \n",
       "5430   And that's how the greatest comedy of TV start...   \n",
       "4072   Lily Mars, a smalltown girl living in Indiana,...   \n",
       "\n",
       "                                              tokenized1  \\\n",
       "8283   [after, seeing, film, i, feel, like, i, know, ...   \n",
       "10937  [my, son, 7, years, old, saw, movie, russian, ...   \n",
       "9347   [remember, early, days, pay, per, view, i, alm...   \n",
       "5430   [and, 's, greatest, comedy, tv, started, it, 1...   \n",
       "4072   [lily, mars, smalltown, girl, living, indiana,...   \n",
       "\n",
       "                                            token_idized  \n",
       "8283   [398, 253, 6, 2, 164, 11, 2, 60, 52, 154, 3831...  \n",
       "10937  [303, 407, 1309, 86, 83, 140, 5, 1959, 7328, 2...  \n",
       "9347   [320, 334, 456, 997, 4080, 642, 2, 147, 320, 5...  \n",
       "5430   [57, 3, 827, 143, 173, 613, 12, 2462, 86, 170,...  \n",
       "4072   [4333, 6196, 1, 168, 555, 9950, 1466, 160, 127...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.iloc[2,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data_frame = csv_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx,self.data_frame[\"token_idized\"][idx])\n",
    "        token_idx = self.data_frame.iloc[idx][\"token_idized\"]\n",
    "        label = self.data_frame.iloc[idx]['labels']\n",
    "#         print(token_idx)\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_fun(batch):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "#     print(batch[0])\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    for datum in batch:\n",
    "        if datum[1]>MAX_SENTENCE_LENGTH:\n",
    "            padded_vec = np.array(datum[0][:MAX_SENTENCE_LENGTH])\n",
    "        else:\n",
    "            padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH - datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "#         print(padded_vec.shape)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.from_numpy(np.array(length_list)), torch.from_numpy(np.array(label_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "train_dataset = IMDBDataset(train_df)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           collate_fn = pad_fun,\n",
    "                                           shuffle = True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_df)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           collate_fn = pad_fun,\n",
    "                                           shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# d = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "class BagOfWords(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super(BagOfWords, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx = 0)\n",
    "#         self.linear = nn.Linear(emb_dim,20)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "#         self.linear2 = nn.Linear(100,300)\n",
    "#         self.linear3 = nn.Linear(300,2)\n",
    "#         self.dp = nn.Dropout(p=0.5)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "#         out = F.relu(self.linear(out.float()))\n",
    "#         out = F.relu(self.linear2(out.float()))\n",
    "#         out = self.linear3(out.float())\n",
    "#         print(out.size())\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = [train_loader,val_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model,criterion, optimizer, name, num_epochs):\n",
    "    best_loss = np.inf\n",
    "    best_acc = 0\n",
    "    loss_hist = {'train':[],'validate':[]}\n",
    "    for i in range(num_epochs):\n",
    "        for enu,phase in enumerate(['train', 'validate']):\n",
    "            running_loss = 0\n",
    "            running_total = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "            for (data, lengths, labels) in dataloaders[enu]:\n",
    "                data_batch, length_batch, label_batch = data.cuda(), lengths.cuda(), labels.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data_batch, length_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                if phase=='train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                N = labels.size(0)\n",
    "                \n",
    "                outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "                predicted = outputs.max(1, keepdim=True)[1]\n",
    "#                 print(type(predicted))\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels.view_as(predicted).cuda()).sum().item()\n",
    "                running_loss += loss.data[0] * N\n",
    "                running_total += N\n",
    "            epoch_loss = running_loss/running_total\n",
    "            loss_hist[phase].append(epoch_loss.item())\n",
    "            accuracy = 100 * correct / total\n",
    "            print('Epoch: {}, Phase: {}, epoch loss: {:.4f}, accuracy: {:.4f}'\\\n",
    "                      .format(i,phase,epoch_loss, accuracy))\n",
    "        if phase == 'validate' and epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_acc = accuracy\n",
    "            torch.save(model,name)\n",
    "    print('Best val dice loss: {:4f}, Best Accuracy: {:4f}'.format(best_loss,best_acc))\n",
    "    return model, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Phase: train, epoch loss: 0.4162, accuracy: 83.3750\n",
      "Epoch: 0, Phase: validate, epoch loss: 0.2715, accuracy: 89.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BagOfWords. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Phase: train, epoch loss: 0.1219, accuracy: 96.9800\n",
      "Epoch: 1, Phase: validate, epoch loss: 0.2760, accuracy: 89.5600\n",
      "Epoch: 2, Phase: train, epoch loss: 0.0349, accuracy: 99.4700\n",
      "Epoch: 2, Phase: validate, epoch loss: 0.3078, accuracy: 89.6000\n",
      "Epoch: 3, Phase: train, epoch loss: 0.0139, accuracy: 99.8900\n",
      "Epoch: 3, Phase: validate, epoch loss: 0.3326, accuracy: 89.6600\n",
      "Epoch: 4, Phase: train, epoch loss: 0.0067, accuracy: 99.9800\n",
      "Epoch: 4, Phase: validate, epoch loss: 0.3564, accuracy: 89.7000\n",
      "Epoch: 5, Phase: train, epoch loss: 0.0039, accuracy: 100.0000\n",
      "Epoch: 5, Phase: validate, epoch loss: 0.3791, accuracy: 89.5600\n",
      "Epoch: 6, Phase: train, epoch loss: 0.0025, accuracy: 100.0000\n",
      "Epoch: 6, Phase: validate, epoch loss: 0.3967, accuracy: 89.6000\n",
      "Epoch: 7, Phase: train, epoch loss: 0.0018, accuracy: 100.0000\n",
      "Epoch: 7, Phase: validate, epoch loss: 0.4130, accuracy: 89.4400\n",
      "Epoch: 8, Phase: train, epoch loss: 0.0013, accuracy: 100.0000\n",
      "Epoch: 8, Phase: validate, epoch loss: 0.4292, accuracy: 89.4600\n",
      "Epoch: 9, Phase: train, epoch loss: 0.0010, accuracy: 100.0000\n",
      "Epoch: 9, Phase: validate, epoch loss: 0.4420, accuracy: 89.4000\n",
      "Best val dice loss: 0.271494, Best Accuracy: 89.740000\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 300\n",
    "model = BagOfWords(len(id2token), emb_dim).cuda()\n",
    "# model = nn.DataParallel(model)\n",
    "learning_rate = 0.01\n",
    "# num_epochs = 100\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "# optimizer = torch.optim.SGD(model)\n",
    "\n",
    "\n",
    "m_save, loss_hists = training(model,criterion,optimizer,\"model19_tokenize2\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2af6239dc7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hists['train'],label=\"train loss\")\n",
    "plt.plot(loss_hists['validate'],label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(loss_hists,open(\"loss_hist19_t2\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset_test(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data_frame = csv_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(self.data_frame.iloc[idx].index)\n",
    "        file_name = self.data_frame.iloc[idx][\"file_names\"]\n",
    "        token_idx = self.data_frame.iloc[idx][\"token_idized\"]\n",
    "        label = self.data_frame.iloc[idx]['labels']\n",
    "        return [token_idx, len(token_idx), label,file_name]\n",
    "\n",
    "\n",
    "def pad_fun_test(batch):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    file_names = []\n",
    "#     print(batch[0])\n",
    "    for datum in batch:\n",
    "        \n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "        file_names.append(datum[3])\n",
    "    for datum in batch:\n",
    "        if datum[1]>MAX_SENTENCE_LENGTH:\n",
    "            padded_vec = np.array(datum[0][:MAX_SENTENCE_LENGTH])\n",
    "        else:\n",
    "            padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH - datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "#         print(padded_vec.shape)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.from_numpy(np.array(length_list)), torch.from_numpy(np.array(label_list)),np.array(file_names)]\n",
    "\n",
    "val_dataset_test = IMDBDataset_test(val_df)\n",
    "val_loader_test = torch.utils.data.DataLoader(dataset = val_dataset_test, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           collate_fn = pad_fun_test,\n",
    "                                           shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_saved = torch.load(\"model19_tokenize2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lengths, labels, file_n = next(iter(val_loader_test))\n",
    "data_batch, length_batch, label_batch = data.cuda(), lengths.cuda(), labels.cuda()\n",
    "outputs = mod_saved(data_batch, length_batch)\n",
    "outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "predicted = outputs.max(1, keepdim=True)[1]\n",
    "mask =(predicted.squeeze(1).eq(label_batch)).cpu().data.numpy()==0\n",
    "fns = file_n[mask]\n",
    "actual_out = labels.data.numpy()[mask]\n",
    "pred_false = predicted.cpu().data.numpy()[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/1217_10.txt\n",
      "I was worried that my daughter might get the wrong idea. I think the \"Dark-Heart\" character is a little on the rough side and I don't like the way he shape-shifts into a \"mean\" frog, fox, boy",
      " I was wrong, This movie was made for my kid, not for me. She \"gets it\" when it went over (under?) my head. Of course I don't \"get it\". This isn't one of the NEW kids movies that adults will ALSO enjoy. This is straight for the young ones, and the crew knew what they were doing. There isn't any political junk ether. There's no magic key that will save the world from ourselves, nobody has the right to access excess, and everyone isn't happy all the time. And as a side benefit, nobody DIES! russwill.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/60_8.txt\n",
      "\"Home Room\" like \"Zero Day\" and \"Elephant\", was inspired by the recent wave of school shootings. But unlike the other two films, \"Home Room\" focuses on two survivors (not the shooters or those killed) in the aftermath of a shooting. Making it less exploitive and more useful because little effort is wasted in asking questions for which there are no answers.<br /><br />Don't give up on this little film during the first 20 minutes, it is supposed to set up the real story but plays like a rejected \"Hill Street Blues\" episode. It is lame but bear with it, at least it pads the running length enough to get the film classified as a feature. I recommend skipping this entirely and just jumping ahead to the hospital scenes-there is nothing here that you can't pick up from the remainder of the film.<br /><br />Like a lot of good little films this was creatively a one-man show as Paul F. Ryan was both the writer and the director. While this arrangement does not guarantee a good film, it is usually a good sign because it will mean a certain unity of construction and execution that is often lacking in big budget dramatic features. Because the script of \"Home Room\" is its real strength it is fortunate that the writer also executed the production and insured that his vision made it onto the screen.<br /><br />Ryan takes a huge chance with the ending which tests the limits of the average viewer's sentimentality tolerance. He runs it right up to the edge but against all logic leaves you crying instead of cringing. Why the ending works is some combination of the audience need for a reward at the end of this kind of journey, the song (Sarah McLaughlin's \"Sweet Surrender\") he goes out on, and the amazing editing of the final minute.<br /><br />The other strength of the film is the casting of Busy Phillips (Alicia) and Erika Christensen (Deanna) as the main protagonists. Although Phillips plays her standard alienated surly teen and Christensen her intelligent daughter of a good family, they both bring more intensity to their roles than ever before. The family life of both girls is more than satisfactory and of little interest to Ryan. What is happening here is all about the two of them despite a lame side story about a police detective wondering around town trying to tie Alicia to the lone shooter. If they ever re-cut and trim the film this side story should be condensed.<br /><br />A story about two extremely disparate girls bonding and helping each other is hardly a novel idea and Ryan could have easily steered this film into clich and predictability. But instead his script has them engaging in a fascinating and convincing sparring match, slowly chipping away at each other and sharing moments of vulnerability, only to retreat back inside themselves. Deanna's \"I'm dying inside\" line just tears you apart-I can't think of a moment in any other film that I felt as intensely as that one. She desperately needs a connection that Alicia just as desperately resists. Deanna only makes progress when she retreats. The viewer keeps expecting the group hug that never seems to happen.<br /><br />Ultimately this not only generates a lot of suspense but leaves you admiring both characters and the two actresses who brought them to life.<br /><br />Then again, what do I know? I'm only a child.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 1\n",
      "Actual 0\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/neg/8439_3.txt\n",
      "I can understand after watching this again for the first time in many years how it is considered one of the worst Laurel & Hardy's. For me, it isn't as close to as bad as \"Air Raid Wardens\" and \"The Bullfighters\", but there are some definite huge flaws in it. The film is set up to show Laurel and Hardy as the owners and instructors of the dance studio. Hardy is funny as the prancing lead of a \"London Bridge\" dance, surrounded by 20th Century Fox starlets, while in the next room, Laurel teaches the beginners ballet while wearing a ballerina outfit. A clumsy carpenter spills glue on the floor, leading to a predicable gag where Hardy ends up the looser. Then, in come the racketeers, now selling insurance covering up their protection racket. One of them is a very young and handsome Robert Mitchum. But no sooner do they bully the boys into buying insurance, they are arrested.<br /><br />This is the end of the gangsters and the last time we see the dance studio. The rest of the film is devoted to Laurel and Hardy's support of wealthy patron Trudy Marshall and her inventor boyfriend, Robert Bailey. They first try to help them hide their relationship from her disapproving parents (Matt Briggs and Margaret Dumont) and hopeful suitor Allan Lane, whom we can tell right off is a no-good swine. This leads to Briggs' hidden bar being revealed to tea-totaling Dumont, and a gag where a rug is literally pulled out from the wealthy patriarch which crashes his bed into a pond below. When Bailey uses the boys to help display his ray gun, pandemonium ensues. The dead-pan butler announces to Case and Dumont that their house is on fire.<br /><br />Later, Hardy wants to use the insurance policy to gain money to pay their dance studio rent and hopes to get Laurel to break a leg to do so. There is no reference to the fact that the insurance salesmen were gangsters and that the policy would probably be invalid. (Even if they were to have become legitimate insurance salesman, after being arrested, their licenses would have been revoked). Laurel ends up getting off a bus which had been abandoned by the driver over a supposedly rabid dog (only a frosting covered, cake devouring Toto look-alike, or possibly the actual pooch), causing Oliver to end up on a huge beach roller-coaster that somehow the bus has ended up on, perfectly fitting its wheels onto the tracks. Roller-coaster gags can be exciting, as evidenced in \"Abbott and Costello Go to Hollywood\", and this one is amusing but anticlimactic.<br /><br />As the story wraps up, all of these gags seem to have no point, giving the impression that this was simply a series of one-reelers put together to make a full-length feature, hopefully part of a double bill. L&H, as I've mentioned in other reviews of their later films, had lost much of their luster after leaving Hal Roach's employ, but surprisingly here, they do not come off as old and tired looking as they had in films made in the same year. Had the gags not been as amusing, as was the case with some of their other films, this surely would have ranked a \"2\" as opposed to a \"3\".\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/6101_8.txt\n",
      "So I don't ruin it for you, I'll be very brief. There's some great acting and funny lines from the attractive cast. A young graduate of Harvard Med School (Brian White) finds out he doesn't know as much as he thinks about people. He goes to a small hospital in Florida for his internship because a girlfriend (Mya) left him for a job as a TV Producer. His Senior Resident (Wood Harris), helped marvelously by his 'creative collaborator'(Zoe Saldana) bring him up to speed. They help protect his career and show him the wider possibilities that come from being a compassionate doctor instead of a player who just wants to make money (as seems to be true for many of my pre-med friends).\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/5210_10.txt\n",
      "Its a shame she didn't get screen credit , she by far did the best job in the film has the girl on the cross , best part of the movie .She had much more impact than Avril , or just about anyone else in the film . She almost made S/M look like fun ! She really was believable has the S/M model that gets scared of her situation . Although they seem to really have messed up that sort of dreamy feeling of looking for the bad guy , Those sets were very well built but they just sort of skimmed the surface of what was shot . This is one film were both cuts should be made available. It seems they left out a lot of what was shot , and almost all of the really dark stuff that would have made the film much more demented . Its kind of like they stopped short of the mark they were going for during filming . What was shot would not have gotten a R rating probably a NC-17 or X , but that is what it would have needed to make the film they way it should have been .\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/7440_7.txt\n",
      "Just saw the movie, and the scary thing was, the people talking during the movie sounded just like the actors. The movie had its moments, but also lagged and was rather sick. It was all meant to be a farce, but once you see the pathetic lives of the people in the movie, you think to yourself \"People like this are all around us\" All attempts at getting the audience's sympathy are dashed as the actors do one stupid thing after another. On the plus side, there are some great (and funny) insults. I think I would wait for video- but it was a good laugh. WARNING- Jerry takes his shirt off during the movie!! (not a pretty sight!)\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/1194_7.txt\n",
      "The film largely focuses on a bullying Robert Taylor as a ruthless buffalo hunter and the people who have to put up with him. Set amidst a hunt for dwindling numbers of buffalo, it portrays the end of a tragic era of senseless slaughter and is full of drama and remorse for both the buffalo and the Native Americans. Taylor is blinded by his hatred of Indians and his naivete that the buffalo herds will never disappear. In one scene, he shoots animal after animal, while in another he murders Indians and then eats the food they had cooking on their fire. Under this ruthless exterior lies an insecure person who is reduced to begging his comrades (Stewart Granger, Lloyd Nolan, and Russ Tamblyn) not to leave him. It's not the most pleasant of films and is weighed down by the drama it creates, leading to a dismal and very fitting conclusion in a blizzard.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 1\n",
      "Actual 0\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/neg/3916_3.txt\n",
      "We all know a movie never does complete justice to the book, but this is exceptional. Important characters were cut out, Blanca and Alba were essentially mushed into the same character, most of the subplots and major elements of the main plot were eliminated. Clara's clairvoyance was extremely downplayed, making her seem like a much more shallow character than the one I got to know in the book. In the book we learn more about her powers and the important effects she had on so many people, which in turn was a key element in the life of the family. In the movie she was no more than some special lady. The relationship between Esteban and Pedro Tercero (Tercero-third-, by the way, is the son and thus comes after Segundo-second-) and its connections to that between Esteban and his grandson from Pancha Garca (not son, who he also did recognize) is chopped in half and its importance downplayed.<br /><br />One of the most fundamental things about the book that the film is all but stripped of: this is called \"The House of the Spirits.\" Where is the house? The story of 3-4 generations of a family is supposed to revolve around the \"big house on the corner,\" a line stated so many times in the novel. The house in fundamental to the story, but the movie unjustly relegates it to a mere backdrop.<br /><br />If I hadn't read the book before, I would have never guessed that such a sappy, shallow movie could be based on such a rich and entertaining novel.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/9270_9.txt\n",
      "I have to point out, before you read this review, that in no way, is this a statement against Iranian people ... if you really want to read something into it, than hopefully you see, that I'm against politicians in general ... but if you're looking to be offended ... I can't help you!<br /><br />Not in Iran as this movie is banned there (see IMDb trivia for this movie). Which is a shame, because the movie is great. Would it not be for \"Grbavica\", this movie would have won at the International Film Festival in Berlin.<br /><br />Rightfully so (it was the runner-up, or second place if you will). Why? Because it is a movie about oppression. It's not even that this is a complete women issue. It is about the government trying to keep the people down. An analogy so clear that the government felt the need to ban the movie. But by banning it nothing is resolved and/or can they make this movie disappear! <br /><br />Another reviewer had a great summary line: \"Comedy about a tragedy\", that sums it up pretty well!\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 1\n",
      "Actual 0\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/neg/2301_3.txt\n",
      "does anyone think that this show actually helps some people, or does it only anger the people who watch it? when i am flipping through the channels and come upon this show i half to watch out of morbid curiosity. i understand that pat Roberson is not all together. what i do not know is if his viewers are like him or if they are good people and think they will have a better life if they listening to what he has to say. pat Roberson is of little consequence. he is an old man who thinks in an old way. fear of damnation no longer has the same affects as it once did (thank god). now if someone will please answer my question i will be dodging lightning bolts for the rest of eternity.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/2040_7.txt\n",
      "CREEP is a straight up serious horror film set in real time that wants nothing more than to just show people get attacked in a empty subway platform by a mutant for 85 minutes. And it does just that. Nothing more, nothing less. Director Christopher Smith draws out the drama a far as he plausibly can by introducing a series of characters that would actually have a reason to be in the subway after it is locked. He also leaves the origins of the titular Creep deliberately vague (unlawful experiments happening in the 60s underground are hinted at) and that little bit of mystery works for the most part. Sadly, he undermines himself toward the end by actually holding back from a twist ending where more genetic malformations would appear (they are hinted at as well). Yes, you heard me right - I wanted a clichd twist ending! Franka (RUN LOLA RUN) Potente is good as the terrorized female lead and the rest of the cast is fine.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 1\n",
      "Actual 0\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/neg/8891_3.txt\n",
      "Easily the best known of all the Shakespeare plays, it has been seriously let down here. Shoddy direction, stagnant studio work and erratic performances spoil a fine tragedy.<br /><br />In the town of Verona, the Capulets and the Montagues have been feuding for centuries but tragedy is imminent when Romeo (Patrick Rycart), a Montague, falls in love with Juliet (Rebecca Saire), a Capulet. Bloodshed soon erupts...<br /><br />The studio work, especially in daytime scenes, seriously stagnates the energy of the play. It's a story that, with it's energy, deserves to be shot outdoors. Coupled with this the costumes are hideous, with too many tights and ludicrous codpieces. The stage fighting looks horrendous, with far too much stretching and running around to be engaging.<br /><br />Patrick Ryecart is too lightweight to be a truly effective Romeo. He manages the character's intensity when the plot gets going but his stately accent and bland, often inexpressive eyes limit his range. It is very hard for the audience to relate to this Romeo. Rebecca Saire is too youthful to be a good Juliet - she captures the character's naivet but a little more sassiness would have been welcome.<br /><br />The supporting roles don't fare much better. Joseph O'Connor's Friar Laurence is fine but too many of his best lines have been cut. Anthony Andrews' Mercutio belongs on stage and not on camera. He gurns and gesticulates excessively and looks rather ridiculous as a result. Alan Rickman, underplaying his role, has virtually no presence as Tybalt. He did develop an edge and intensity to deliver some fine screen performances in later years, but that isn't in evidence here. The Prince can be a fine role with his brief appearances but actor Lawrence Naismith fails to give the part any authority on camera. Only Micheal Hordern, in probably his best role in this series, comes out of this with any dignity. His Capulet is well-played and a joy to watch.<br /><br />See one of the other versions of this story instead.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/6999_7.txt\n",
      "Seven Pounds stars Will Smith as Ben Thomas, an IRS collection agent who has an ulterior motive for meeting those who have gotten behind on their tax payments. Thomas caused the deaths of seven people whilst driving talking on his phone and the movie follows his attempt to try and atone for his, quite frankly, unforgivable crime.<br /><br />The story line is as subtle as a brick through a green house window. What you see is exactly what is happening - even the ending of Titanic was more surprising when compared to Seven Pounds. There is absolutely no twist whatsoever, there is never any confusion or doubt as to what is happening.<br /><br />Normally I only like Will Smith when is in full Bad Boys mode. The guy is at his best when sporting a gun, driving a Ford GT and saying \"Aw Hell Nah\" as whenever he tries to act serious it only comes across as a pathetic attempt at trying to gain an Oscar which he is so obviously is desperate for. This is probably the main reason why I wasn't looking forward to the movie, although here he is very understated. There are flashes of comedy but they are subtle, in fact it was the most understated Will Smith performance I have ever seen and for that reason alone he was fantastic and ironically should be nominated for a major award.<br /><br />The supporting cast were all grand too - Rosario Dawson looked pretty much at deaths door the whole film and Woody Harrelson and Barry Pepper were fine as where all the other bit players.<br /><br />Overall it is a weepy - but it isn't throwing all the usual clichs and sentimental violins to give you no other choice but to cry. There were a few times that even my hardened heart nearly broke. You will be hard pressed not to find one of the situations that does not relate to your own life which makes it seem all the more real.<br /><br />I would give it 8/10. The Will Smith show moves onto drama without all the desperation of The Pursuit of Happiness and comes of all the better for it.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 1\n",
      "Actual 0\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/neg/88_2.txt\n",
      "I very much looked forward to this movie. Its a good family movie; however, if Michael Landon Jr.'s editing team did a better job of editing, the movie would be much better. Too many scenes out of context. I do hope there is another movie from the series, they're all very good. But, if another one is made, I beg them to take better care at editing. This story was all over the place and didn't seem to have a center. Which is unfortunate because the other movies of the series were great. I enjoy the story of Willie and Missy; they're both great role models. Plus, the romantic side of the viewers always enjoy a good love story.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(fns)):\n",
    "    print(\"predicted\",pred_false[i][0])\n",
    "    print(\"Actual\",actual_out[i])\n",
    "    print(val_df[val_df['file_names'] ==fns[i]][\"file_names\"].values[0])\n",
    "    f = open(val_df[val_df['file_names'] ==fns[i]][\"file_names\"].values[0])\n",
    "    print(f.read())\n",
    "    print()\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
