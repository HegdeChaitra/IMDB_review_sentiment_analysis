{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "MAX_SENTENCE_LENGTH = 400\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"i\",\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"to\",\"from\",\"up\",\"down\",\"in\",\"on\",\"off\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"other\",\"some\",\"such\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "tokenize = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_pos = \"/home/cvh255/nlp_hw1/aclImdb/train/pos/\"\n",
    "path_train_neg = \"/home/cvh255/nlp_hw1/aclImdb/train/neg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_files = os.listdir(path_train_pos)\n",
    "for f in range(len(train_pos_files)):\n",
    "    train_pos_files[f] = path_train_pos + train_pos_files[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_files = os.listdir(path_train_neg)\n",
    "for f in range(len(train_neg_files)):\n",
    "    train_neg_files[f] = path_train_neg + train_neg_files[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_labels = [1]*len(train_pos_files)\n",
    "train_neg_labels = [0]*len(train_neg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"file_names\",\"labels\"])\n",
    "df[\"file_names\"] = train_pos_files+train_neg_files\n",
    "df[\"labels\"] = train_pos_labels+train_neg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9258_10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3_10.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9597_10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3347_7.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/2160_8.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          file_names  labels\n",
       "0  /home/cvh255/nlp_hw1/aclImdb/train/pos/9258_10...       1\n",
       "1    /home/cvh255/nlp_hw1/aclImdb/train/pos/3_10.txt       1\n",
       "2  /home/cvh255/nlp_hw1/aclImdb/train/pos/9597_10...       1\n",
       "3  /home/cvh255/nlp_hw1/aclImdb/train/pos/3347_7.txt       1\n",
       "4  /home/cvh255/nlp_hw1/aclImdb/train/pos/2160_8.txt       1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(df[\"file_names\"], df[\"labels\"]):\n",
    "    train_df = df.iloc[train_index]\n",
    "    val_df = df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 2), (5000, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape,val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "    all_txt = []\n",
    "    for i,j in df.iterrows():\n",
    "        f = open(j[\"file_names\"])\n",
    "        txt = f.read()\n",
    "        all_txt.append(txt)\n",
    "#         print(j)\n",
    "    df[\"content\"] = all_txt\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_df = get_data(train_df)\n",
    "val_df = get_data(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation\n",
    "def tokenize1(phrase):\n",
    "    tokens = tokenize(phrase)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations and token.text not in stop_words)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = list(nltk.trigrams(tokenize1(\"I am going mad! you are nuts\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'going', 'mad'), ('going', 'mad', 'nuts')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i going mad', 'going mad nuts']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[' '.join(a) for a in bg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset1(dataset,n_gram):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize1(sample)\n",
    "        bg = list(nltk.bigrams(tokens))\n",
    "        tg = list(nltk.trigrams(tokens))\n",
    "        bg_t = [' '.join(a) for a in bg]\n",
    "        tg_t = [' '.join(a) for a in tg]\n",
    "        tokens = tokens + bg_t + tg_t\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens+=tokens\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing val data\")\n",
    "val_df[\"tokenized1\"], _ = tokenize_dataset1(val_df[\"content\"],1)\n",
    "# pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_df[\"tokenized1\"], all_train_tokens = tokenize_dataset1(train_df[\"content\"],1)\n",
    "# pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>content</th>\n",
       "      <th>tokenized1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8283</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>After seeing this film I feel like I know just...</td>\n",
       "      <td>[after, seeing, film, i, feel, like, i, know, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...</td>\n",
       "      <td>1</td>\n",
       "      <td>My son was 7 years old when he saw this movie,...</td>\n",
       "      <td>[my, son, 7, years, old, saw, movie, russian, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9347</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Remember the early days of Pay Per View? I do,...</td>\n",
       "      <td>[remember, early, days, pay, per, view, i, alm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...</td>\n",
       "      <td>1</td>\n",
       "      <td>And that's how the greatest comedy of TV start...</td>\n",
       "      <td>[and, 's, greatest, comedy, tv, started, it, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Lily Mars, a smalltown girl living in Indiana,...</td>\n",
       "      <td>[lily, mars, smalltown, girl, living, indiana,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_names  labels  \\\n",
       "8283   /home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt       1   \n",
       "10937  /home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...       1   \n",
       "9347   /home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt       1   \n",
       "5430   /home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...       1   \n",
       "4072   /home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt       1   \n",
       "\n",
       "                                                 content  \\\n",
       "8283   After seeing this film I feel like I know just...   \n",
       "10937  My son was 7 years old when he saw this movie,...   \n",
       "9347   Remember the early days of Pay Per View? I do,...   \n",
       "5430   And that's how the greatest comedy of TV start...   \n",
       "4072   Lily Mars, a smalltown girl living in Indiana,...   \n",
       "\n",
       "                                              tokenized1  \n",
       "8283   [after, seeing, film, i, feel, like, i, know, ...  \n",
       "10937  [my, son, 7, years, old, saw, movie, russian, ...  \n",
       "9347   [remember, early, days, pay, per, view, i, alm...  \n",
       "5430   [and, 's, greatest, comedy, tv, started, it, 1...  \n",
       "4072   [lily, mars, smalltown, girl, living, indiana,...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_df_tok4.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.to_csv(\"val_df_tok4.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(all_train_tokens,open(\"all_tokens4\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_df_tok1.csv\")\n",
    "val_df = pd.read_csv(\"val_df_tok1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "all_train_tokens = pickle.load(open(\"all_tokens1\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 200000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 198693 ; token the brief\n",
      "Token the brief; token id 198693\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_df['token_idized'] = token2index_dataset(train_df['tokenized1'])\n",
    "val_df['token_idized'] = token2index_dataset(val_df['tokenized1'])\n",
    "# test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>content</th>\n",
       "      <th>tokenized1</th>\n",
       "      <th>token_idized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8283</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>After seeing this film I feel like I know just...</td>\n",
       "      <td>[after, seeing, film, i, feel, like, i, know, ...</td>\n",
       "      <td>[398, 253, 6, 2, 164, 11, 2, 60, 52, 154, 3831...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...</td>\n",
       "      <td>1</td>\n",
       "      <td>My son was 7 years old when he saw this movie,...</td>\n",
       "      <td>[my, son, 7, years, old, saw, movie, russian, ...</td>\n",
       "      <td>[303, 407, 1309, 86, 83, 140, 5, 1959, 7328, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9347</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Remember the early days of Pay Per View? I do,...</td>\n",
       "      <td>[remember, early, days, pay, per, view, i, alm...</td>\n",
       "      <td>[320, 334, 456, 997, 4080, 642, 2, 147, 320, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...</td>\n",
       "      <td>1</td>\n",
       "      <td>And that's how the greatest comedy of TV start...</td>\n",
       "      <td>[and, 's, greatest, comedy, tv, started, it, 1...</td>\n",
       "      <td>[57, 3, 827, 143, 173, 613, 12, 2462, 86, 170,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Lily Mars, a smalltown girl living in Indiana,...</td>\n",
       "      <td>[lily, mars, smalltown, girl, living, indiana,...</td>\n",
       "      <td>[4333, 6196, 1, 168, 555, 9950, 1466, 160, 127...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_names  labels  \\\n",
       "8283   /home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt       1   \n",
       "10937  /home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...       1   \n",
       "9347   /home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt       1   \n",
       "5430   /home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...       1   \n",
       "4072   /home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt       1   \n",
       "\n",
       "                                                 content  \\\n",
       "8283   After seeing this film I feel like I know just...   \n",
       "10937  My son was 7 years old when he saw this movie,...   \n",
       "9347   Remember the early days of Pay Per View? I do,...   \n",
       "5430   And that's how the greatest comedy of TV start...   \n",
       "4072   Lily Mars, a smalltown girl living in Indiana,...   \n",
       "\n",
       "                                              tokenized1  \\\n",
       "8283   [after, seeing, film, i, feel, like, i, know, ...   \n",
       "10937  [my, son, 7, years, old, saw, movie, russian, ...   \n",
       "9347   [remember, early, days, pay, per, view, i, alm...   \n",
       "5430   [and, 's, greatest, comedy, tv, started, it, 1...   \n",
       "4072   [lily, mars, smalltown, girl, living, indiana,...   \n",
       "\n",
       "                                            token_idized  \n",
       "8283   [398, 253, 6, 2, 164, 11, 2, 60, 52, 154, 3831...  \n",
       "10937  [303, 407, 1309, 86, 83, 140, 5, 1959, 7328, 2...  \n",
       "9347   [320, 334, 456, 997, 4080, 642, 2, 147, 320, 5...  \n",
       "5430   [57, 3, 827, 143, 173, 613, 12, 2462, 86, 170,...  \n",
       "4072   [4333, 6196, 1, 168, 555, 9950, 1466, 160, 127...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.iloc[2,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data_frame = csv_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx,self.data_frame[\"token_idized\"][idx])\n",
    "        token_idx = self.data_frame.iloc[idx][\"token_idized\"]\n",
    "        label = self.data_frame.iloc[idx]['labels']\n",
    "#         print(token_idx)\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_fun(batch):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "#     print(batch[0])\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    for datum in batch:\n",
    "        if datum[1]>MAX_SENTENCE_LENGTH:\n",
    "            padded_vec = np.array(datum[0][:MAX_SENTENCE_LENGTH])\n",
    "        else:\n",
    "            padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH - datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "#         print(padded_vec.shape)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.from_numpy(np.array(length_list)), torch.from_numpy(np.array(label_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "train_dataset = IMDBDataset(train_df)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           collate_fn = pad_fun,\n",
    "                                           shuffle = True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_df)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           collate_fn = pad_fun,\n",
    "                                           shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# d = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "class BagOfWords(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super(BagOfWords, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx = 0)\n",
    "#         self.linear = nn.Linear(emb_dim,20)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "#         self.linear2 = nn.Linear(100,300)\n",
    "#         self.linear3 = nn.Linear(300,2)\n",
    "#         self.dp = nn.Dropout(p=0.5)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "#         out = F.relu(self.linear(out.float()))\n",
    "#         out = F.relu(self.linear2(out.float()))\n",
    "#         out = self.linear3(out.float())\n",
    "#         print(out.size())\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = [train_loader,val_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model,criterion, optimizer, name, num_epochs):\n",
    "    best_loss = np.inf\n",
    "    best_acc = 0\n",
    "    loss_hist = {'train':[],'validate':[]}\n",
    "    for i in range(num_epochs):\n",
    "        for enu,phase in enumerate(['train', 'validate']):\n",
    "            running_loss = 0\n",
    "            running_total = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "            for (data, lengths, labels) in dataloaders[enu]:\n",
    "                data_batch, length_batch, label_batch = data.cuda(), lengths.cuda(), labels.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data_batch, length_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                if phase=='train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                N = labels.size(0)\n",
    "                \n",
    "                outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "                predicted = outputs.max(1, keepdim=True)[1]\n",
    "#                 print(type(predicted))\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels.view_as(predicted).cuda()).sum().item()\n",
    "                running_loss += loss.data[0] * N\n",
    "                running_total += N\n",
    "            epoch_loss = running_loss/running_total\n",
    "            loss_hist[phase].append(epoch_loss.item())\n",
    "            accuracy = 100 * correct / total\n",
    "            print('Epoch: {}, Phase: {}, epoch loss: {:.4f}, accuracy: {:.4f}'\\\n",
    "                      .format(i,phase,epoch_loss, accuracy))\n",
    "        if phase == 'validate' and epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_acc = accuracy\n",
    "            torch.save(model,name)\n",
    "    print('Best val dice loss: {:4f}, Best Accuracy: {:4f}'.format(best_loss,best_acc))\n",
    "    return model, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Phase: train, epoch loss: 0.4129, accuracy: 83.2300\n",
      "Epoch: 0, Phase: validate, epoch loss: 0.2888, accuracy: 88.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BagOfWords. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Phase: train, epoch loss: 0.0990, accuracy: 97.7350\n",
      "Epoch: 1, Phase: validate, epoch loss: 0.2640, accuracy: 90.2400\n",
      "Epoch: 2, Phase: train, epoch loss: 0.0218, accuracy: 99.7800\n",
      "Epoch: 2, Phase: validate, epoch loss: 0.2798, accuracy: 90.3400\n",
      "Epoch: 3, Phase: train, epoch loss: 0.0079, accuracy: 99.9700\n",
      "Epoch: 3, Phase: validate, epoch loss: 0.2961, accuracy: 90.3600\n",
      "Epoch: 4, Phase: train, epoch loss: 0.0041, accuracy: 100.0000\n",
      "Epoch: 4, Phase: validate, epoch loss: 0.3126, accuracy: 90.4800\n",
      "Epoch: 5, Phase: train, epoch loss: 0.0025, accuracy: 100.0000\n",
      "Epoch: 5, Phase: validate, epoch loss: 0.3263, accuracy: 90.3800\n",
      "Epoch: 6, Phase: train, epoch loss: 0.0016, accuracy: 100.0000\n",
      "Epoch: 6, Phase: validate, epoch loss: 0.3391, accuracy: 90.2800\n",
      "Epoch: 7, Phase: train, epoch loss: 0.0012, accuracy: 100.0000\n",
      "Epoch: 7, Phase: validate, epoch loss: 0.3497, accuracy: 90.2800\n",
      "Epoch: 8, Phase: train, epoch loss: 0.0009, accuracy: 100.0000\n",
      "Epoch: 8, Phase: validate, epoch loss: 0.3596, accuracy: 90.3200\n",
      "Epoch: 9, Phase: train, epoch loss: 0.0007, accuracy: 100.0000\n",
      "Epoch: 9, Phase: validate, epoch loss: 0.3695, accuracy: 90.1200\n",
      "Best val dice loss: 0.264010, Best Accuracy: 90.240000\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 200\n",
    "model = BagOfWords(len(id2token), emb_dim).cuda()\n",
    "# model = nn.DataParallel(model)\n",
    "learning_rate = 0.01\n",
    "# num_epochs = 100\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "# optimizer = torch.optim.SGD(model)\n",
    "\n",
    "\n",
    "m_save, loss_hists = training(model,criterion,optimizer,\"model21_tokenize2\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2af5ec5e5860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hists['train'],label=\"train loss\")\n",
    "plt.plot(loss_hists['validate'],label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(loss_hists,open(\"loss_hist21_t2\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset_test(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data_frame = csv_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(self.data_frame.iloc[idx].index)\n",
    "        file_name = self.data_frame.iloc[idx][\"file_names\"]\n",
    "        token_idx = self.data_frame.iloc[idx][\"token_idized\"]\n",
    "        label = self.data_frame.iloc[idx]['labels']\n",
    "        return [token_idx, len(token_idx), label,file_name]\n",
    "\n",
    "\n",
    "def pad_fun_test(batch):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    file_names = []\n",
    "#     print(batch[0])\n",
    "    for datum in batch:\n",
    "        \n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "        file_names.append(datum[3])\n",
    "    for datum in batch:\n",
    "        if datum[1]>MAX_SENTENCE_LENGTH:\n",
    "            padded_vec = np.array(datum[0][:MAX_SENTENCE_LENGTH])\n",
    "        else:\n",
    "            padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH - datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "#         print(padded_vec.shape)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.from_numpy(np.array(length_list)), torch.from_numpy(np.array(label_list)),np.array(file_names)]\n",
    "\n",
    "val_dataset_test = IMDBDataset_test(val_df)\n",
    "val_loader_test = torch.utils.data.DataLoader(dataset = val_dataset_test, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           collate_fn = pad_fun_test,\n",
    "                                           shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_saved = torch.load(\"model21_tokenize2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lengths, labels, file_n = next(iter(val_loader_test))\n",
    "data_batch, length_batch, label_batch = data.cuda(), lengths.cuda(), labels.cuda()\n",
    "outputs = mod_saved(data_batch, length_batch)\n",
    "outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "predicted = outputs.max(1, keepdim=True)[1]\n",
    "mask =(predicted.squeeze(1).eq(label_batch)).cpu().data.numpy()==0\n",
    "fns = file_n[mask]\n",
    "actual_out = labels.data.numpy()[mask]\n",
    "pred_false = predicted.cpu().data.numpy()[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/5565_8.txt\n",
      "I'm a fan of C&C, going back to their records, and liked this movie, but at one point in the mid-1980's on cable television in San Jose California, it was aired with an alternate plot line that destroyed the entire point of the movie. All references to marijuana were replaced with \"diamonds\". The bag that \"Red\" drops to Chong has diamonds in it instead of marijuana, but the conversation still remains the same (\"...it's worth ~$3000/lb\"). There is also a subplot in which clips of aliens on a ship were added observing C&C, and talking to each other about getting the diamonds. At the end, instead of \"space coke\", it's something else. I'm not sure who created this version, but it was horrible, and obvious that they were attempting to make it family/child friendly. It would have been better if that network had not aired it at all.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 1\n",
      "Actual 0\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/neg/1757_1.txt\n",
      "Don't get me wrong, Dan Jansen was a great speed skater. If there was one guy who deserved his gold medal at the Olympics it was Dan.<br /><br />But how can it be possible that Bill Corcoran has made such a bad movie about the incredible Dan Jansen story, because the real Dan Jansen story is truly incredible! Especially when you look at this movie through the eyes of a sportsman everything is wrong, the way Matt Keeslar and the other actors skate, their technique, the dimensions of the speedskating oval, it is all wrong!<br /><br />Shame on you, Bill Corcoran, Dan Jansen deserves better, a lot better!<br /><br />1 out of 10\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/803_10.txt\n",
      "As Most Off You Might off Seen Star Wars: Return Off The Jedi You May Knows Its A Good Movie But As You Might Have Seen On Video They M|might have a party At The end And They Just Probably End The Movie with the party with no a spirits or anything But on the original one (Live TV) When they are Partying But before i say more when Ben obi-wan dies in the Imperial Ship Or Death Star They Saw him Disappear And Yoda Dies From Either Old Age Or Internal Illness But because Luke killed Darth Vader (Real Name: Anakin Skywalker) When They All Are Partying At The end when Luke Or Someone Stops the Spirits Off Ben And Yoda Stands Starring At Him And Smiling While Another Spirit Appears Is its Darth Vader but not as A Sith As The Old Usual Selve off Him And Started Smiling with Ben And Yoda I reckon That made the movie ending a little bit interesting But the Producers or anyone should off made a spirit off Padme And Mace Windu And Other Jedis that got killed with Younglings Under There Arms in the back ground\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/815_7.txt\n",
      "This film is to my mind the weakest film in the original Star Wars trilogy, for a variety of reasons. However it emerges at the end of the day a winner, despite all its flaws. It's still a very good film, even if a lot of its quality depends on the characters that have been built up in the superior 2 installments.<br /><br />One problem here is the look of the film, which isn't very consistent with the other 2 films. I put a lot of that down to the departure of producer Gary Kurtz. The first 2 films have that dirty, lived-in look with all the technology and so forth. In \"Jedi\" on the other hand even the rebels look like they just stepped out of a shower and had their uniforms dry cleaned. This makes for a much less textured film. Also the creatures were excessively muppet-like and cutesy. At this point it seems like the film-makers were more concerned with creating the templates for future action figures than with the quality of the film itself.<br /><br />Another aspect is its lack of originality. Where \"Star Wars\" created a whole new experience in cinema and \"Empire\" brought us to alien worlds of swamps, ice, and clouds, \"Jedi\" lamely re-cycled the locations of the first film. First we are back on the desert planet Tatooine, and then we are watching them face ANOTHER death star (maybe the emperor couldn't think of anything new... but you'd think Lucas or Kasdan could). Also we have these ewoks, who really are just detestable made-for-mattel teddy bears, in a recycled version of what was supposed to be the big wookie-fight at the end of \"Star Wars\" if they hadn't run out of cash. It just feels like lazy construction.<br /><br />The most unfortunate aspect of \"Jedi\" for me is the weak handling of the Han Solo character. Whereas he is central to the plot of the first 2 films here he is struggling for screen time, trading one liners with the droids. Instead of a real drama we're stuck with the lame pretense that Han is still convinced Leia loves Luke -- as if the conclusion of \"Empire\" where she confessed her love of him had never happened. The whole thing is very contrived and barely conceals the fact that the Solo character was not part of this film's central story after his rescue. Ford, for his part, looks bored and lacks the style that distinguished his earlier performances. This is more like a 1990s Ford performance, bored and looking \"above\" the film itself. Fisher for her part is visibly high in some scenes. Lando, an interesting character introduced in \"Empire\", here is stuck as the ostensible person we care about in the giant space battle. Only Hamill, given an interesting development in the Luke character, is really able to do anything new or interesting with his character. Probably he was the only major actor in the film who still cared about his work. And to be fair the script gives him a lot more to do than the other characters. Really it is his story and the other characters are only there as part of the package. Ian McDiarmid does excellent work as well as the Emperor. The film would sink if he had been too far over the top (as he was at times in the new films).<br /><br />Visually and in terms of effects work, other than the \"clean\" look of everything it's hard to find fault. Jabba is a very effective animatronic character, one of the most elaborate ever constructed. The space battles towards the end are very impressive.<br /><br />Ultimately this film coasts to success based on the accomplishments of its forebears. But on its own, it is a satisfying piece of entertainment and IMHO far superior to any of Lucas' later productions.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/3090_10.txt\n",
      "I am so upset that ABC is giving up on yet another show that has the chance to be a real winner. This show is so good, the writing and storyline were great, an actual original idea for a show instead of another boring reality show. The casting was spectacular! Not only were the characters and actors right on, but these are a very talented set of actors. The concept and idea is really a new and cool idea for a TV show, many of us share this whole idea of \"connections\". I really love the characters of Steven, Laura, Whitney and Damien. But to be honest there is not one person connected to this show that I did not like, even those that only were in for a few episodes (Sheri Appleby for example). The acting and characters are so easy to like and so talented!!. I wish ABC had given this show more of a chance, and not interrupted the show midway, Also it was not advertised enough. Truly unfair!! to everyone!!. This show showed great promise. I for one will let ABC know how I feel and will keep sending emails to keep this show alive. Please join me, I know we can do it. It worked for Jerico. (By the way where is episode 13? I want that last show!). Please support this show and send emails to ABC,we can do it!!! This show is well worth it!!! PETITION ONLINE TO SAVE SIX DEGREES: http://www.PetitionOnline.com/gh1215/petition.html WE CAN DO IT!! SIGN THIS PETITION ASAP!!!\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/2646_10.txt\n",
      "I think the majority of the people seem not the get the right idea about the movie, at least that's my opinion. I am not sure it's a movie about drug abuse; rather it's a movie about the way of thinking of those genius brothers, drugs are side effects, something marginal. Again, it's not a commercial movie that you see every day and if the author wanted that, he definitely failed, as most people think it's one of the many drug related movies. I, however, think something else is the case. As in many movies portraying different cultures, audience usually fully understands movies portraying their own culture, i.e. something they've grown up with and are quite familiar with. This movie is to show what those \"genius\" people very often think and what problems they face. The reason why they act like this is because they are bored out of their minds :) They have to meet people who do mediocre things and accept those things as if they are launching space shuttles on daily basis. They start a fairly hard job and excel in no time. They feel like- I went to work, did nothing, still did twice as better as the guys around me when they were all over their projects, what should I do now with my free time. And what's even more boring? When you can start predicting behavior not because you're psychologist, but instead because you have seen this pattern in the past. So, for them, from one side it's a non challenging job, which is also fairly boring sometimes, and from another they start to figure out people's behavior. It's a recipe for big big boredom. And the dumbest things are usually done to get out of this state. They guy earlier who mentioned that their biggest problem is that they are trying to figure out life in terms of logic (math describes logic), while life is not really a logical thing, is actually absolutely right.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(fns)):\n",
    "    print(\"predicted\",pred_false[i][0])\n",
    "    print(\"Actual\",actual_out[i])\n",
    "    print(val_df[val_df['file_names'] ==fns[i]][\"file_names\"].values[0])\n",
    "    f = open(val_df[val_df['file_names'] ==fns[i]][\"file_names\"].values[0])\n",
    "    print(f.read())\n",
    "    print()\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
