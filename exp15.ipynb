{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "MAX_SENTENCE_LENGTH = 500\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"i\",\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"to\",\"from\",\"up\",\"down\",\"in\",\"on\",\"off\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"other\",\"some\",\"such\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "tokenize = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_pos = \"/home/cvh255/nlp_hw1/aclImdb/train/pos/\"\n",
    "path_train_neg = \"/home/cvh255/nlp_hw1/aclImdb/train/neg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_files = os.listdir(path_train_pos)\n",
    "for f in range(len(train_pos_files)):\n",
    "    train_pos_files[f] = path_train_pos + train_pos_files[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_files = os.listdir(path_train_neg)\n",
    "for f in range(len(train_neg_files)):\n",
    "    train_neg_files[f] = path_train_neg + train_neg_files[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_labels = [1]*len(train_pos_files)\n",
    "train_neg_labels = [0]*len(train_neg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"file_names\",\"labels\"])\n",
    "df[\"file_names\"] = train_pos_files+train_neg_files\n",
    "df[\"labels\"] = train_pos_labels+train_neg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9258_10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3_10.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9597_10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3347_7.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/2160_8.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          file_names  labels\n",
       "0  /home/cvh255/nlp_hw1/aclImdb/train/pos/9258_10...       1\n",
       "1    /home/cvh255/nlp_hw1/aclImdb/train/pos/3_10.txt       1\n",
       "2  /home/cvh255/nlp_hw1/aclImdb/train/pos/9597_10...       1\n",
       "3  /home/cvh255/nlp_hw1/aclImdb/train/pos/3347_7.txt       1\n",
       "4  /home/cvh255/nlp_hw1/aclImdb/train/pos/2160_8.txt       1"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(df[\"file_names\"], df[\"labels\"]):\n",
    "    train_df = df.iloc[train_index]\n",
    "    val_df = df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 2), (5000, 2))"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape,val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "    all_txt = []\n",
    "    for i,j in df.iterrows():\n",
    "        f = open(j[\"file_names\"])\n",
    "        txt = f.read()\n",
    "        all_txt.append(txt)\n",
    "#         print(j)\n",
    "    df[\"content\"] = all_txt\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_df = get_data(train_df)\n",
    "val_df = get_data(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation\n",
    "def tokenize1(phrase):\n",
    "    tokens = tokenize(phrase)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations and token.text not in stop_words)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = list(nltk.bigrams(tokenize1(\"I am going mad! you are nuts\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'going'), ('going', 'mad'), ('mad', 'nuts')]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i going', 'going mad', 'mad nuts']"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[' '.join(a) for a in bg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset1(dataset,n_gram):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize1(sample)\n",
    "        bg = list(nltk.bigrams(tokens))\n",
    "        bg_t = [' '.join(a) for a in bg]\n",
    "        tokens = tokens + bg_t\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens+=tokens\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing val data\")\n",
    "val_df[\"tokenized1\"], _ = tokenize_dataset1(val_df[\"content\"],1)\n",
    "# pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_df[\"tokenized1\"], all_train_tokens = tokenize_dataset1(train_df[\"content\"],1)\n",
    "# pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>content</th>\n",
       "      <th>tokenized1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8283</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>After seeing this film I feel like I know just...</td>\n",
       "      <td>[after, seeing, film, i, feel, like, i, know, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...</td>\n",
       "      <td>1</td>\n",
       "      <td>My son was 7 years old when he saw this movie,...</td>\n",
       "      <td>[my, son, 7, years, old, saw, movie, russian, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9347</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Remember the early days of Pay Per View? I do,...</td>\n",
       "      <td>[remember, early, days, pay, per, view, i, alm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...</td>\n",
       "      <td>1</td>\n",
       "      <td>And that's how the greatest comedy of TV start...</td>\n",
       "      <td>[and, 's, greatest, comedy, tv, started, it, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Lily Mars, a smalltown girl living in Indiana,...</td>\n",
       "      <td>[lily, mars, smalltown, girl, living, indiana,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_names  labels  \\\n",
       "8283   /home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt       1   \n",
       "10937  /home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...       1   \n",
       "9347   /home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt       1   \n",
       "5430   /home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...       1   \n",
       "4072   /home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt       1   \n",
       "\n",
       "                                                 content  \\\n",
       "8283   After seeing this film I feel like I know just...   \n",
       "10937  My son was 7 years old when he saw this movie,...   \n",
       "9347   Remember the early days of Pay Per View? I do,...   \n",
       "5430   And that's how the greatest comedy of TV start...   \n",
       "4072   Lily Mars, a smalltown girl living in Indiana,...   \n",
       "\n",
       "                                              tokenized1  \n",
       "8283   [after, seeing, film, i, feel, like, i, know, ...  \n",
       "10937  [my, son, 7, years, old, saw, movie, russian, ...  \n",
       "9347   [remember, early, days, pay, per, view, i, alm...  \n",
       "5430   [and, 's, greatest, comedy, tv, started, it, 1...  \n",
       "4072   [lily, mars, smalltown, girl, living, indiana,...  "
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_df_tok3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.to_csv(\"val_df_tok3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(all_train_tokens,open(\"all_tokens3\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_df_tok1.csv\")\n",
    "val_df = pd.read_csv(\"val_df_tok1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "all_train_tokens = pickle.load(open(\"all_tokens1\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 50000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 33075 ; token says nothing\n",
      "Token says nothing; token id 33075\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_df['token_idized'] = token2index_dataset(train_df['tokenized1'])\n",
    "val_df['token_idized'] = token2index_dataset(val_df['tokenized1'])\n",
    "# test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>content</th>\n",
       "      <th>tokenized1</th>\n",
       "      <th>token_idized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8283</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>After seeing this film I feel like I know just...</td>\n",
       "      <td>[after, seeing, film, i, feel, like, i, know, ...</td>\n",
       "      <td>[398, 253, 6, 2, 164, 11, 2, 60, 52, 154, 3775...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...</td>\n",
       "      <td>1</td>\n",
       "      <td>My son was 7 years old when he saw this movie,...</td>\n",
       "      <td>[my, son, 7, years, old, saw, movie, russian, ...</td>\n",
       "      <td>[303, 407, 1292, 86, 83, 140, 5, 1938, 7135, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9347</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Remember the early days of Pay Per View? I do,...</td>\n",
       "      <td>[remember, early, days, pay, per, view, i, alm...</td>\n",
       "      <td>[320, 334, 456, 989, 4016, 638, 2, 147, 320, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...</td>\n",
       "      <td>1</td>\n",
       "      <td>And that's how the greatest comedy of TV start...</td>\n",
       "      <td>[and, 's, greatest, comedy, tv, started, it, 1...</td>\n",
       "      <td>[57, 3, 822, 143, 173, 610, 12, 2435, 86, 170,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Lily Mars, a smalltown girl living in Indiana,...</td>\n",
       "      <td>[lily, mars, smalltown, girl, living, indiana,...</td>\n",
       "      <td>[4258, 6052, 1, 168, 553, 9632, 1448, 160, 127...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_names  labels  \\\n",
       "8283   /home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt       1   \n",
       "10937  /home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...       1   \n",
       "9347   /home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt       1   \n",
       "5430   /home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...       1   \n",
       "4072   /home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt       1   \n",
       "\n",
       "                                                 content  \\\n",
       "8283   After seeing this film I feel like I know just...   \n",
       "10937  My son was 7 years old when he saw this movie,...   \n",
       "9347   Remember the early days of Pay Per View? I do,...   \n",
       "5430   And that's how the greatest comedy of TV start...   \n",
       "4072   Lily Mars, a smalltown girl living in Indiana,...   \n",
       "\n",
       "                                              tokenized1  \\\n",
       "8283   [after, seeing, film, i, feel, like, i, know, ...   \n",
       "10937  [my, son, 7, years, old, saw, movie, russian, ...   \n",
       "9347   [remember, early, days, pay, per, view, i, alm...   \n",
       "5430   [and, 's, greatest, comedy, tv, started, it, 1...   \n",
       "4072   [lily, mars, smalltown, girl, living, indiana,...   \n",
       "\n",
       "                                            token_idized  \n",
       "8283   [398, 253, 6, 2, 164, 11, 2, 60, 52, 154, 3775...  \n",
       "10937  [303, 407, 1292, 86, 83, 140, 5, 1938, 7135, 2...  \n",
       "9347   [320, 334, 456, 989, 4016, 638, 2, 147, 320, 5...  \n",
       "5430   [57, 3, 822, 143, 173, 610, 12, 2435, 86, 170,...  \n",
       "4072   [4258, 6052, 1, 168, 553, 9632, 1448, 160, 127...  "
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.iloc[2,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data_frame = csv_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx,self.data_frame[\"token_idized\"][idx])\n",
    "        token_idx = self.data_frame.iloc[idx][\"token_idized\"]\n",
    "        label = self.data_frame.iloc[idx]['labels']\n",
    "#         print(token_idx)\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_fun(batch):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "#     print(batch[0])\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    for datum in batch:\n",
    "        if datum[1]>MAX_SENTENCE_LENGTH:\n",
    "            padded_vec = np.array(datum[0][:MAX_SENTENCE_LENGTH])\n",
    "        else:\n",
    "            padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH - datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "#         print(padded_vec.shape)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.from_numpy(np.array(length_list)), torch.from_numpy(np.array(label_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "train_dataset = IMDBDataset(train_df)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           collate_fn = pad_fun,\n",
    "                                           shuffle = True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_df)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           collate_fn = pad_fun,\n",
    "                                           shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# d = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "class BagOfWords(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super(BagOfWords, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx = 0)\n",
    "#         self.linear = nn.Linear(emb_dim,20)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "#         self.linear2 = nn.Linear(100,300)\n",
    "#         self.linear3 = nn.Linear(300,2)\n",
    "#         self.dp = nn.Dropout(p=0.5)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "#         out = F.relu(self.linear(out.float()))\n",
    "#         out = F.relu(self.linear2(out.float()))\n",
    "#         out = self.linear3(out.float())\n",
    "#         print(out.size())\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = [train_loader,val_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model,criterion, optimizer, name, num_epochs):\n",
    "    best_loss = np.inf\n",
    "    best_acc = 0\n",
    "    loss_hist = {'train':[],'validate':[]}\n",
    "    for i in range(num_epochs):\n",
    "        for enu,phase in enumerate(['train', 'validate']):\n",
    "            running_loss = 0\n",
    "            running_total = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "            for (data, lengths, labels) in dataloaders[enu]:\n",
    "                data_batch, length_batch, label_batch = data.cuda(), lengths.cuda(), labels.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data_batch, length_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                if phase=='train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                N = labels.size(0)\n",
    "                \n",
    "                outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "                predicted = outputs.max(1, keepdim=True)[1]\n",
    "#                 print(type(predicted))\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels.view_as(predicted).cuda()).sum().item()\n",
    "                running_loss += loss.data[0] * N\n",
    "                running_total += N\n",
    "            epoch_loss = running_loss/running_total\n",
    "            loss_hist[phase].append(epoch_loss.item())\n",
    "            accuracy = 100 * correct / total\n",
    "            print('Epoch: {}, Phase: {}, epoch loss: {:.4f}, accuracy: {:.4f}'\\\n",
    "                      .format(i,phase,epoch_loss, accuracy))\n",
    "        if phase == 'validate' and epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_acc = accuracy\n",
    "            torch.save(model,name)\n",
    "    print('Best val dice loss: {:4f}, Best Accuracy: {:4f}'.format(best_loss,best_acc))\n",
    "    return model, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Phase: train, epoch loss: 0.3758, accuracy: 85.1750\n",
      "Epoch: 0, Phase: validate, epoch loss: 0.2625, accuracy: 90.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BagOfWords. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Phase: train, epoch loss: 0.1078, accuracy: 97.3650\n",
      "Epoch: 1, Phase: validate, epoch loss: 0.2802, accuracy: 89.7200\n",
      "Epoch: 2, Phase: train, epoch loss: 0.0264, accuracy: 99.7450\n",
      "Epoch: 2, Phase: validate, epoch loss: 0.3299, accuracy: 89.6800\n",
      "Epoch: 3, Phase: train, epoch loss: 0.0071, accuracy: 99.9800\n",
      "Epoch: 3, Phase: validate, epoch loss: 0.3655, accuracy: 89.5800\n",
      "Epoch: 4, Phase: train, epoch loss: 0.0029, accuracy: 100.0000\n",
      "Epoch: 4, Phase: validate, epoch loss: 0.3893, accuracy: 89.5400\n",
      "Epoch: 5, Phase: train, epoch loss: 0.0017, accuracy: 100.0000\n",
      "Epoch: 5, Phase: validate, epoch loss: 0.4109, accuracy: 89.5400\n",
      "Epoch: 6, Phase: train, epoch loss: 0.0011, accuracy: 100.0000\n",
      "Epoch: 6, Phase: validate, epoch loss: 0.4263, accuracy: 89.6400\n",
      "Epoch: 7, Phase: train, epoch loss: 0.0008, accuracy: 100.0000\n",
      "Epoch: 7, Phase: validate, epoch loss: 0.4411, accuracy: 89.7000\n",
      "Epoch: 8, Phase: train, epoch loss: 0.0006, accuracy: 100.0000\n",
      "Epoch: 8, Phase: validate, epoch loss: 0.4540, accuracy: 89.6600\n",
      "Epoch: 9, Phase: train, epoch loss: 0.0005, accuracy: 100.0000\n",
      "Epoch: 9, Phase: validate, epoch loss: 0.4665, accuracy: 89.6600\n",
      "Best val dice loss: 0.262497, Best Accuracy: 90.100000\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 300\n",
    "model = BagOfWords(len(id2token), emb_dim).cuda()\n",
    "# model = nn.DataParallel(model)\n",
    "learning_rate = 0.01\n",
    "# num_epochs = 100\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "# optimizer = torch.optim.SGD(model)\n",
    "\n",
    "\n",
    "m_save, loss_hists = training(model,criterion,optimizer,\"model18_tokenize2\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b15ba8eae80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hists['train'],label=\"train loss\")\n",
    "plt.plot(loss_hists['validate'],label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(loss_hists,open(\"loss_hist18_t2\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset_test(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data_frame = csv_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(self.data_frame.iloc[idx].index)\n",
    "        file_name = self.data_frame.iloc[idx][\"file_names\"]\n",
    "        token_idx = self.data_frame.iloc[idx][\"token_idized\"]\n",
    "        label = self.data_frame.iloc[idx]['labels']\n",
    "        return [token_idx, len(token_idx), label,file_name]\n",
    "\n",
    "\n",
    "def pad_fun_test(batch):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    file_names = []\n",
    "#     print(batch[0])\n",
    "    for datum in batch:\n",
    "        \n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "        file_names.append(datum[3])\n",
    "    for datum in batch:\n",
    "        if datum[1]>MAX_SENTENCE_LENGTH:\n",
    "            padded_vec = np.array(datum[0][:MAX_SENTENCE_LENGTH])\n",
    "        else:\n",
    "            padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH - datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "#         print(padded_vec.shape)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.from_numpy(np.array(length_list)), torch.from_numpy(np.array(label_list)),np.array(file_names)]\n",
    "\n",
    "val_dataset_test = IMDBDataset_test(val_df)\n",
    "val_loader_test = torch.utils.data.DataLoader(dataset = val_dataset_test, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           collate_fn = pad_fun_test,\n",
    "                                           shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_saved = torch.load(\"model18_tokenize2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lengths, labels, file_n = next(iter(val_loader_test))\n",
    "data_batch, length_batch, label_batch = data.cuda(), lengths.cuda(), labels.cuda()\n",
    "outputs = mod_saved(data_batch, length_batch)\n",
    "outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "predicted = outputs.max(1, keepdim=True)[1]\n",
    "mask =(predicted.squeeze(1).eq(label_batch)).cpu().data.numpy()==0\n",
    "fns = file_n[mask]\n",
    "actual_out = labels.data.numpy()[mask]\n",
    "pred_false = predicted.cpu().data.numpy()[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted 1\n",
      "Actual 0\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/neg/10865_2.txt\n",
      "I love the munna bhai MBBS but \"Lagge raho...\" SUX really SUX. I have never seen such a boring movie in my whole life. And these high ratings really astonished me that wat happened to the taste of Indian cinema viewers ?? <br /><br />**MAY BE SPOILER** <br /><br />An educated girl needs an advice from a Bhai, people discussing their personal prob. on phones come on man from which part of the world u r ??? I agree that films should be fictitious but these things are really indigestible.<br /><br />2 out of 10. (2 stars is for 15 mins good starting)\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/578_10.txt\n",
      "There is not much more I can say about this movie than all of the commentaries on page one, except - as Jesse says - \"it's the berries\". All of page one's commentators wrote eloquently - as almost so as the dialog is in this movie. This just may be one of those illusive things we hear so much about, but usually are made so by the actors who deliver the lines: a beautiful script. Maybe Robert Redford did hold strict sway onto the actors/actresses during the filming of this movie, so that the beauty of the story would not get lost.<br /><br />I, too, attended church when I was very young and into my late teens. The church's pastor spoke very eloquently and quietly as the Rev. Maclean did in his church. That, in itself, is a totally different picture that is portrayed of Southern Baptist churches - no holy-rollering in my church. It was a big church, with many different programs to keep its congregation busy - the most inspiring perhaps was the music-department with its huge choir and almost classical anthems. Too, the Sunday evening-congregation was almost entirely younger people. Are you even aware it was once safe to go to church on Sunday night? How I wish it still was ! Watching \"A River Runs through It\" is very much like going to hear a beautiful sermon in a church whose members are fully involved in life. As has already been so beautifully written, the sermon for this movie is the open-space beauties of Montana - yet, aren't there also missile-silos there, too? Fly-fishing or any other activity which draws family-members closer together for a happy life - and deep understanding of one another - becomes a blessing. Although you see some of the shadier aspects of life then, too, the simplicity of the story paints a lasting impression on your heart, if you let it. Speakeasies and prostitutes are counter-balanced by the simple gatherings of old-fashioned, community picnics as this movie contains - in heavy contrasts to modern families taking their kids to Disney Land for screeching joyrides and calling it \"a day together\". There is noting wrong with that, but as \"River\" demonstrates, some of its taciturn beauty could do nothing but make life richer. This is the third film I've seen in which Tom Skeritt (?) plays a father, all different styles and brilliantly acted.<br /><br />Brad Pitt, mostly an undiscovered talent except for \"Thelma and Louise\" and \"Meet Joe Black\", and all of the cast-members deserved many awards. Little stories superbly told will get 10-of-10 from me over any movie with violence, foul language, ugliness and \"action\". I am thinking particularly of \"Crash\" and most of \"Arnold's\" movies. What a savior for peace this movie is.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/10456_10.txt\n",
      "This movie is not about entertainment, or not even a movie you want to see to pass the time. This movie is a genuinely a display of true love that can only come from God. One cannot help but be touched deeply by looking at this movie. We have several dimensions of love that contributes to the value of this movie. There is the divine love of God that is beautifully portrayed. God's love transcends the heart and mind and endures and is eternal. There is the love in a marriage. While the main character grapples with his wife's disease, he realizes through God's love that he loves his wife more than he could ever imagine. He knows that he and his wife are one and can never be separated. Finally, you have the love of child and parent. The kids in the family come together and realize that nothing else matters except that love conquers fear. Dear friends, love is not love unless it comes from God, because God is love and love comes from God. Talk to someone and let them know you love them. Love does no good unless it is given to another. I pray this movie can inspire and change the lives of everyone who sees it. Amen!!\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 1\n",
      "Actual 0\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/neg/5274_3.txt\n",
      "If you like movies about creepy towns, hotels, houses, states (ala the Eagles \"Hotel California\"), etc. that possess the people that are \"just passing through,\" read almost any Stephen King novel instead. If you like the setting of \"Disappearance\" start by reading King's \"Desperation\" but also check out \"The Shining\", \"Salem's Lot\" and \"Needful Things.\"<br /><br />The crow motif, the desert, the family driving in desperation to escape or avoid possession are tired. Why didn't they just make the film from the \"Desperation\" novel? Maybe they approached King and he nixed? Must be.<br /><br />Susan Dey and Harry Hamlin look happy to be reunited and they have both worn well over the years, but they're still TV and direct-to-DVD caliber actors.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/5385_7.txt\n",
      "I never watched the 'Next Action Hero' show, and until reading the other comments here, did not know that this movie was the 'prize' from that competition. I was just flipping channels and came across this, and found myself watching, dare eagerly, all the way to the end.<br /><br />Yes, the plot's been done (The Most Dangerous Game, etc.) but I was hoping for, and almost received, the 'gotcha' - how the protagonist was going to beat the hunter in the end. I think the high-tech was overdone (GPS's) and gave me cold-sweat flashbacks of Night Rider, but it nevertheless was not too overdone.<br /><br />The basic problem I had with this movie was the degree of SOD (Suspension of Disbelief) that was required of the viewer. Do we really think that someone flying in a helicopter could lob countless incendiary grenades at a public bridge and NO COPS show up to investigate? Could a limousine do countless donuts in a Las Vegas intersection and NO COPS show up? Pleeease. Way too much of that type of thing - fun to watch, but keep it at least plausible, thank you very much.<br /><br />The final solution was good, but the ending was disappointing, with the after taste of a bad Star Trek episode. At least now I understand why the acting was so cheesy - except for Zane, who doesn't get near as much work as he deserves IMHO - they were winners from a reality show.<br /><br />Knock me out.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/10886_10.txt\n",
      "When I think of the cheesiest guilty pleasure-type movies, the first thing I think of are '80s slasher flicks. Really bad slasher flicks. The formulaic type of film, where all a script needed was 2 parts blood and several parts nudity to get made.<br /><br />Flash forward to the late '90s/early '00s. The slasher flick has been revitalized with the success of 1996's \"Scream\". Like in the '80s, these films were formulaic, masking a lack of inspiration by labelling themselves as \"hip, tongue-in-cheek parodies\" of the original slasher flicks. Of this recent blend of \"hip parody\" neo-slasher flicks, the only one worth seeing is the low-budget, direct-to-video \"Cut\".<br /><br />Like most of the other \"new\" slasher flicks, \"Cut\" relies on the production of a slasher flick, in this case a fictional 1985 film \"Hot Blooded\", to make its commentary on the genre. \"Hot Blooded\" never finished production, because of killings by someone wearing the mask of the film's killer, Scarman, a bald figure with its mouth stitched close and dark, pupil-less eyes. Now, 12 years later, a group of film students, whose professor was involved in the production, have decided to go into the vaults, tap the original surviving actress, and finish the film. But every time the film is screened or a scene is shot, \"Scarman\" returns and someone dies. To quote the tagline, will they finish the film before it finishes them?<br /><br />This all sounds really bad, and to a degree it is (really, is there such a thing as a good slasher flick?). There is no character development (the \"new\" director is revealed to be the daughter of \"Hot Blooded\"'s original director, whose life was apparently ruined after the production was cancelled; this would've been a perfect detail to be worked into the plot, yet it's never mentioned again) and, like in all other slasher flicks, there are just too many bodies to care about. The actors aren't great, even by direct-to-video standards, but most are having fun with their characters (and for those who aren't, it's inadvertent character acting, since none of their characters in the film wanted to work on \"Hot Blooded\"), particularly whoever was lucky enough to play Scarman. \"Cut\"'s climax has no big \"who dunnit\" unmasking of the killer like in the \"Scream\" films. It doesn't have the gimmick killings of the \"Urban Legend\" films. What it does have is an original and interesting concept that is diluted by a \"this way we can write a sequel if it sells well\" ending. But that's par for the course.<br /><br />By any sensible viewing standards, this is a horrible movie that should be avoided, but this \"quality\" is what makes it true to its roots in the slasher genre, and this is what makes it more enjoyable than any of the other neo-slasher flicks.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 1\n",
      "Actual 0\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/neg/7779_4.txt\n",
      "\"True\" story of a late monster that appears when an American industrial plant begins polluting the waters. Amusing, though not really good, monster film has lots of people trying to get the monster and find out whats going on but not in a completely involving way. Give it points for giving us a giant monster that they clearly built to scale for some scenes but take some away in that it looks like a non threatening puppy. An amusing exploitation film thats enjoyably silly in the right frame of mind. (My one complaint is that the print used on the Elvira release is so poor that it looks like a well worn video tape copy that was past its prime 20 years ago.)\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 1\n",
      "Actual 0\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/neg/1418_4.txt\n",
      "I don't know why this conduct was ever tolerated in the movie business! This movie (short) is gross (to say the least)! It is a bunch of 5-7 year old children wearing diapers with big bobby pins, acting like adults (and too much so!). However, it is interesting because it is a good example of how \"the good old days\" may not have been so good after all! (Thank GOD we have laws against this kind of material now!)<br /><br />{This is one short from the \"Shirley Temple Festival\"}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/9957_7.txt\n",
      "This film is shockingly underrated on IMDb. Like so many films, this isn't Shawshank. But it's a reasonably good, if predictable, dance competition / personal growth film. If you want to spend an hour and a half watching a sort of 8 Mile for a female step dancer, than I think you'll like it.<br /><br />Judging from the IMDb ratings, my guess is that this movie was approaching the top 250, and was \"vote bombed\" with many 1s, as happens to so many films that aren't about the mob, don't have special effects, or include non-white or non-straight characters.<br /><br />It's an American film, but it's not a US film. Set mostly in Toronto the cues are subtle, and some audiences may think it's set entirely in the US just because the final competition is in the border city of Detroit.<br /><br />I liked the music. I liked the dance (but not convinced it's worth $50,000 ... but what do I know). The characters were easy on the eyes.<br /><br />I do agree the title sucks. I don't remember anyone in the film saying those words, and it should have an \"s\". (No, it's not a foreign language).<br /><br />There's not a lot to hate about this film (and let's be honest, a vote of 1 means you hated it) so I can only assume that it's an expression of hate for the kind of people in it, and that's sad.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/8387_8.txt\n",
      "This first part of the BRD Trilogy has more passion and plot density than Lola, but less of the magic of Veronica Voss. The political musings have point to them: we see the shortages after the war, how the blackmarketers were able to control so much of the day-to-day life (delicious moment when Fassbinder, playing a grifter, tries to sell a complete set of Kleist to Schygulla, who remarks that burning books don't provide much warmth: she really wants firewood).<br /><br />There's some clumsiness in the first hour. The scene in Maria's room with the black soldier, interrupted by Hermann's appearance should go quicker. The train scene when Maria meets Karl Oswald falls flat when she insults the GI--I cringed, it was so bad. But as the story develops and the years go by, I was drawn more and more into this glossy, cold world.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 1\n",
      "Actual 0\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/neg/7273_4.txt\n",
      "This early role for Barbara Shelley(in fact,her first in Britain after working in Italy),was made when she was 24 years old,and it's certainly safe to say that she made a stunning debut in 1957's \"Cat Girl.\" While blondes and brunettes get most of the attention(I'll always cherish Yutte Stensgaard),the lovely auburn-haired actress with the deep voice always exuded intelligence as well as vulnerability(one such example being 1960's \"Village of the Damned,\" in which her screen time was much less than her character's husband,George Sanders).She is the sole reason for seeing this drab update of \"Cat People,\" and is seen to great advantage throughout(it's difficult to say if her beauty found an even better showcase).Her character apparently sleeps in the nude,and we are exposed to her luscious bare back when she is awakened(also exposed 8 years later in 1965's \"Rasputin-The Mad Monk\").The ravishing gown she wears during most of the film is a stunning strapless wonder(I don't see what held that dress up,but I'd sure like to).All in all,proof positive that Barbara Shelley,in a poorly written role that would defeat most actresses,rises above her material and makes the film consistently watchable,a real test of star power,which she would find soon enough at Hammer's studios in Bray,for the duration of the 1960's.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(fns)):\n",
    "    print(\"predicted\",pred_false[i][0])\n",
    "    print(\"Actual\",actual_out[i])\n",
    "    print(val_df[val_df['file_names'] ==fns[i]][\"file_names\"].values[0])\n",
    "    f = open(val_df[val_df['file_names'] ==fns[i]][\"file_names\"].values[0])\n",
    "    print(f.read())\n",
    "    print()\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
