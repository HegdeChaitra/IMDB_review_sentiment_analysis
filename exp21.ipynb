{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "MAX_SENTENCE_LENGTH = 300\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"i\",\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"to\",\"from\",\"up\",\"down\",\"in\",\"on\",\"off\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"other\",\"some\",\"such\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "tokenize = spacy.load('en_core_web_sm')\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_pos = \"/home/cvh255/nlp_hw1/aclImdb/train/pos/\"\n",
    "path_train_neg = \"/home/cvh255/nlp_hw1/aclImdb/train/neg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_files = os.listdir(path_train_pos)\n",
    "for f in range(len(train_pos_files)):\n",
    "    train_pos_files[f] = path_train_pos + train_pos_files[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_files = os.listdir(path_train_neg)\n",
    "for f in range(len(train_neg_files)):\n",
    "    train_neg_files[f] = path_train_neg + train_neg_files[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_labels = [1]*len(train_pos_files)\n",
    "train_neg_labels = [0]*len(train_neg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"file_names\",\"labels\"])\n",
    "df[\"file_names\"] = train_pos_files+train_neg_files\n",
    "df[\"labels\"] = train_pos_labels+train_neg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9258_10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3_10.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9597_10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3347_7.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/2160_8.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          file_names  labels\n",
       "0  /home/cvh255/nlp_hw1/aclImdb/train/pos/9258_10...       1\n",
       "1    /home/cvh255/nlp_hw1/aclImdb/train/pos/3_10.txt       1\n",
       "2  /home/cvh255/nlp_hw1/aclImdb/train/pos/9597_10...       1\n",
       "3  /home/cvh255/nlp_hw1/aclImdb/train/pos/3347_7.txt       1\n",
       "4  /home/cvh255/nlp_hw1/aclImdb/train/pos/2160_8.txt       1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(df[\"file_names\"], df[\"labels\"]):\n",
    "    train_df = df.iloc[train_index]\n",
    "    val_df = df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 2), (5000, 2))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape,val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "    all_txt = []\n",
    "    for i,j in df.iterrows():\n",
    "        f = open(j[\"file_names\"])\n",
    "        txt = f.read()\n",
    "        all_txt.append(txt)\n",
    "#         print(j)\n",
    "    df[\"content\"] = all_txt\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_df = get_data(train_df)\n",
    "val_df = get_data(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation\n",
    "def tokenize1(phrase):\n",
    "    tokens = tokenize(phrase)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations and token.text not in stop_words)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = list(ngrams(tokenize1(\"I am going mad! you are nuts and mad again\"),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'going', 'mad', 'nuts'), ('going', 'mad', 'nuts', 'mad')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i going mad', 'going mad nuts']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[' '.join(a) for a in bg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset1(dataset,n_gram):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize1(sample)\n",
    "        bg = list(nltk.bigrams(tokens))\n",
    "        tg = list(nltk.trigrams(tokens))\n",
    "        fg = list(ngrams(tokens,4))\n",
    "        bg_t = [' '.join(a) for a in bg]\n",
    "        tg_t = [' '.join(a) for a in tg]\n",
    "        fg_t = [' '.join(a) for a in fg]\n",
    "        tokens = tokens + bg_t + tg_t + fg_t\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens+=tokens\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing val data\")\n",
    "val_df[\"tokenized1\"], _ = tokenize_dataset1(val_df[\"content\"],1)\n",
    "# pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_df[\"tokenized1\"], all_train_tokens = tokenize_dataset1(train_df[\"content\"],1)\n",
    "# pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>content</th>\n",
       "      <th>tokenized1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8283</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>After seeing this film I feel like I know just...</td>\n",
       "      <td>[after, seeing, film, i, feel, like, i, know, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...</td>\n",
       "      <td>1</td>\n",
       "      <td>My son was 7 years old when he saw this movie,...</td>\n",
       "      <td>[my, son, 7, years, old, saw, movie, russian, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9347</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Remember the early days of Pay Per View? I do,...</td>\n",
       "      <td>[remember, early, days, pay, per, view, i, alm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...</td>\n",
       "      <td>1</td>\n",
       "      <td>And that's how the greatest comedy of TV start...</td>\n",
       "      <td>[and, 's, greatest, comedy, tv, started, it, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Lily Mars, a smalltown girl living in Indiana,...</td>\n",
       "      <td>[lily, mars, smalltown, girl, living, indiana,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_names  labels  \\\n",
       "8283   /home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt       1   \n",
       "10937  /home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...       1   \n",
       "9347   /home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt       1   \n",
       "5430   /home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...       1   \n",
       "4072   /home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt       1   \n",
       "\n",
       "                                                 content  \\\n",
       "8283   After seeing this film I feel like I know just...   \n",
       "10937  My son was 7 years old when he saw this movie,...   \n",
       "9347   Remember the early days of Pay Per View? I do,...   \n",
       "5430   And that's how the greatest comedy of TV start...   \n",
       "4072   Lily Mars, a smalltown girl living in Indiana,...   \n",
       "\n",
       "                                              tokenized1  \n",
       "8283   [after, seeing, film, i, feel, like, i, know, ...  \n",
       "10937  [my, son, 7, years, old, saw, movie, russian, ...  \n",
       "9347   [remember, early, days, pay, per, view, i, alm...  \n",
       "5430   [and, 's, greatest, comedy, tv, started, it, 1...  \n",
       "4072   [lily, mars, smalltown, girl, living, indiana,...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_df_tok5.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.to_csv(\"val_df_tok5.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(all_train_tokens,open(\"all_tokens5\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_df_tok1.csv\")\n",
    "val_df = pd.read_csv(\"val_df_tok1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "all_train_tokens = pickle.load(open(\"all_tokens1\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 100000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 5306 ; token jealous\n",
      "Token jealous; token id 5306\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_df['token_idized'] = token2index_dataset(train_df['tokenized1'])\n",
    "val_df['token_idized'] = token2index_dataset(val_df['tokenized1'])\n",
    "# test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>labels</th>\n",
       "      <th>content</th>\n",
       "      <th>tokenized1</th>\n",
       "      <th>token_idized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8283</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>After seeing this film I feel like I know just...</td>\n",
       "      <td>[after, seeing, film, i, feel, like, i, know, ...</td>\n",
       "      <td>[398, 253, 6, 2, 164, 11, 2, 60, 52, 154, 3834...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...</td>\n",
       "      <td>1</td>\n",
       "      <td>My son was 7 years old when he saw this movie,...</td>\n",
       "      <td>[my, son, 7, years, old, saw, movie, russian, ...</td>\n",
       "      <td>[303, 407, 1309, 86, 83, 140, 5, 1960, 7343, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9347</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Remember the early days of Pay Per View? I do,...</td>\n",
       "      <td>[remember, early, days, pay, per, view, i, alm...</td>\n",
       "      <td>[320, 334, 456, 997, 4083, 642, 2, 147, 320, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...</td>\n",
       "      <td>1</td>\n",
       "      <td>And that's how the greatest comedy of TV start...</td>\n",
       "      <td>[and, 's, greatest, comedy, tv, started, it, 1...</td>\n",
       "      <td>[57, 3, 827, 143, 173, 613, 12, 2463, 86, 170,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>/home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Lily Mars, a smalltown girl living in Indiana,...</td>\n",
       "      <td>[lily, mars, smalltown, girl, living, indiana,...</td>\n",
       "      <td>[4337, 6205, 1, 168, 555, 9991, 1467, 160, 127...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_names  labels  \\\n",
       "8283   /home/cvh255/nlp_hw1/aclImdb/train/pos/4793_7.txt       1   \n",
       "10937  /home/cvh255/nlp_hw1/aclImdb/train/pos/11592_1...       1   \n",
       "9347   /home/cvh255/nlp_hw1/aclImdb/train/pos/3243_8.txt       1   \n",
       "5430   /home/cvh255/nlp_hw1/aclImdb/train/pos/10129_7...       1   \n",
       "4072   /home/cvh255/nlp_hw1/aclImdb/train/pos/9873_7.txt       1   \n",
       "\n",
       "                                                 content  \\\n",
       "8283   After seeing this film I feel like I know just...   \n",
       "10937  My son was 7 years old when he saw this movie,...   \n",
       "9347   Remember the early days of Pay Per View? I do,...   \n",
       "5430   And that's how the greatest comedy of TV start...   \n",
       "4072   Lily Mars, a smalltown girl living in Indiana,...   \n",
       "\n",
       "                                              tokenized1  \\\n",
       "8283   [after, seeing, film, i, feel, like, i, know, ...   \n",
       "10937  [my, son, 7, years, old, saw, movie, russian, ...   \n",
       "9347   [remember, early, days, pay, per, view, i, alm...   \n",
       "5430   [and, 's, greatest, comedy, tv, started, it, 1...   \n",
       "4072   [lily, mars, smalltown, girl, living, indiana,...   \n",
       "\n",
       "                                            token_idized  \n",
       "8283   [398, 253, 6, 2, 164, 11, 2, 60, 52, 154, 3834...  \n",
       "10937  [303, 407, 1309, 86, 83, 140, 5, 1960, 7343, 2...  \n",
       "9347   [320, 334, 456, 997, 4083, 642, 2, 147, 320, 5...  \n",
       "5430   [57, 3, 827, 143, 173, 613, 12, 2463, 86, 170,...  \n",
       "4072   [4337, 6205, 1, 168, 555, 9991, 1467, 160, 127...  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.iloc[2,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data_frame = csv_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx,self.data_frame[\"token_idized\"][idx])\n",
    "        token_idx = self.data_frame.iloc[idx][\"token_idized\"]\n",
    "        label = self.data_frame.iloc[idx]['labels']\n",
    "#         print(token_idx)\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_fun(batch):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "#     print(batch[0])\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    for datum in batch:\n",
    "        if datum[1]>MAX_SENTENCE_LENGTH:\n",
    "            padded_vec = np.array(datum[0][:MAX_SENTENCE_LENGTH])\n",
    "        else:\n",
    "            padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH - datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "#         print(padded_vec.shape)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.from_numpy(np.array(length_list)), torch.from_numpy(np.array(label_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "train_dataset = IMDBDataset(train_df)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           collate_fn = pad_fun,\n",
    "                                           shuffle = True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_df)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           collate_fn = pad_fun,\n",
    "                                           shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# d = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "class BagOfWords(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super(BagOfWords, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx = 0)\n",
    "#         self.linear = nn.Linear(emb_dim,20)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "#         self.linear2 = nn.Linear(100,300)\n",
    "#         self.linear3 = nn.Linear(300,2)\n",
    "#         self.dp = nn.Dropout(p=0.5)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "#         out = F.relu(self.linear(out.float()))\n",
    "#         out = F.relu(self.linear2(out.float()))\n",
    "#         out = self.linear3(out.float())\n",
    "#         print(out.size())\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = [train_loader,val_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model,criterion, optimizer, name, num_epochs):\n",
    "    best_loss = np.inf\n",
    "    best_acc = 0\n",
    "    loss_hist = {'train':[],'validate':[]}\n",
    "    for i in range(num_epochs):\n",
    "        for enu,phase in enumerate(['train', 'validate']):\n",
    "            running_loss = 0\n",
    "            running_total = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "            for (data, lengths, labels) in dataloaders[enu]:\n",
    "                data_batch, length_batch, label_batch = data.cuda(), lengths.cuda(), labels.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data_batch, length_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                if phase=='train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                N = labels.size(0)\n",
    "                \n",
    "                outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "                predicted = outputs.max(1, keepdim=True)[1]\n",
    "#                 print(type(predicted))\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels.view_as(predicted).cuda()).sum().item()\n",
    "                running_loss += loss.data[0] * N\n",
    "                running_total += N\n",
    "            epoch_loss = running_loss/running_total\n",
    "            loss_hist[phase].append(epoch_loss.item())\n",
    "            accuracy = 100 * correct / total\n",
    "            print('Epoch: {}, Phase: {}, epoch loss: {:.4f}, accuracy: {:.4f}'\\\n",
    "                      .format(i,phase,epoch_loss, accuracy))\n",
    "        if phase == 'validate' and epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_acc = accuracy\n",
    "            torch.save(model,name)\n",
    "    print('Best val dice loss: {:4f}, Best Accuracy: {:4f}'.format(best_loss,best_acc))\n",
    "    return model, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Phase: train, epoch loss: 0.4717, accuracy: 79.1150\n",
      "Epoch: 0, Phase: validate, epoch loss: 0.2880, accuracy: 88.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvh255/pyenv/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BagOfWords. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Phase: train, epoch loss: 0.1437, accuracy: 96.1350\n",
      "Epoch: 1, Phase: validate, epoch loss: 0.2554, accuracy: 90.3600\n",
      "Epoch: 2, Phase: train, epoch loss: 0.0456, accuracy: 99.2950\n",
      "Epoch: 2, Phase: validate, epoch loss: 0.2799, accuracy: 90.1400\n",
      "Epoch: 3, Phase: train, epoch loss: 0.0170, accuracy: 99.8600\n",
      "Epoch: 3, Phase: validate, epoch loss: 0.2948, accuracy: 90.6200\n",
      "Epoch: 4, Phase: train, epoch loss: 0.0080, accuracy: 99.9950\n",
      "Epoch: 4, Phase: validate, epoch loss: 0.3139, accuracy: 90.6600\n",
      "Epoch: 5, Phase: train, epoch loss: 0.0047, accuracy: 100.0000\n",
      "Epoch: 5, Phase: validate, epoch loss: 0.3309, accuracy: 90.5600\n",
      "Epoch: 6, Phase: train, epoch loss: 0.0030, accuracy: 100.0000\n",
      "Epoch: 6, Phase: validate, epoch loss: 0.3457, accuracy: 90.3800\n",
      "Epoch: 7, Phase: train, epoch loss: 0.0021, accuracy: 100.0000\n",
      "Epoch: 7, Phase: validate, epoch loss: 0.3587, accuracy: 90.4200\n",
      "Epoch: 8, Phase: train, epoch loss: 0.0015, accuracy: 100.0000\n",
      "Epoch: 8, Phase: validate, epoch loss: 0.3703, accuracy: 90.3600\n",
      "Epoch: 9, Phase: train, epoch loss: 0.0012, accuracy: 100.0000\n",
      "Epoch: 9, Phase: validate, epoch loss: 0.3810, accuracy: 90.3400\n",
      "Best val dice loss: 0.255446, Best Accuracy: 90.360000\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 300\n",
    "model = BagOfWords(len(id2token), emb_dim).cuda()\n",
    "# model = nn.DataParallel(model)\n",
    "learning_rate = 0.01\n",
    "# num_epochs = 100\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "# optimizer = torch.optim.SGD(model)\n",
    "\n",
    "\n",
    "m_save, loss_hists = training(model,criterion,optimizer,\"model24_tokenize2\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2af5eca8ec88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hists['train'],label=\"train loss\")\n",
    "plt.plot(loss_hists['validate'],label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(loss_hists,open(\"loss_hist24_t2\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset_test(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data_frame = csv_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(self.data_frame.iloc[idx].index)\n",
    "        file_name = self.data_frame.iloc[idx][\"file_names\"]\n",
    "        token_idx = self.data_frame.iloc[idx][\"token_idized\"]\n",
    "        label = self.data_frame.iloc[idx]['labels']\n",
    "        return [token_idx, len(token_idx), label,file_name]\n",
    "\n",
    "\n",
    "def pad_fun_test(batch):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    file_names = []\n",
    "#     print(batch[0])\n",
    "    for datum in batch:\n",
    "        \n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "        file_names.append(datum[3])\n",
    "    for datum in batch:\n",
    "        if datum[1]>MAX_SENTENCE_LENGTH:\n",
    "            padded_vec = np.array(datum[0][:MAX_SENTENCE_LENGTH])\n",
    "        else:\n",
    "            padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH - datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "#         print(padded_vec.shape)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.from_numpy(np.array(length_list)), torch.from_numpy(np.array(label_list)),np.array(file_names)]\n",
    "\n",
    "val_dataset_test = IMDBDataset_test(val_df)\n",
    "val_loader_test = torch.utils.data.DataLoader(dataset = val_dataset_test, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           collate_fn = pad_fun_test,\n",
    "                                           shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_saved = torch.load(\"model24_tokenize2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lengths, labels, file_n = next(iter(val_loader_test))\n",
    "data_batch, length_batch, label_batch = data.cuda(), lengths.cuda(), labels.cuda()\n",
    "outputs = mod_saved(data_batch, length_batch)\n",
    "outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "predicted = outputs.max(1, keepdim=True)[1]\n",
    "mask =(predicted.squeeze(1).eq(label_batch)).cpu().data.numpy()==0\n",
    "fns = file_n[mask]\n",
    "actual_out = labels.data.numpy()[mask]\n",
    "pred_false = predicted.cpu().data.numpy()[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/5205_7.txt\n",
      "A bunch of American students and their tutor decide to visit the ugliest part of Ireland in order to study ancient religious practices. Despite being repeatedly warned about the dangers of straying off the beaten path (by the local creepy Irish guy, natch), they do just that, and wind up with their insides on the outside courtesy of a family of inbred cannibals (the descendants of the infamous Sawney Bean clan, who according to the film's silly plot, upped sticks from Scotland and settled on the Emerald Isle).<br /><br />If you think that porn stars plus low budget horror automatically equals tons of nudity and terrible acting, then think again: Evil Breed is bristling with adult stars, but in fact, there's not nearly as much nudity as one might expect given the 'talent' involved, and the acting, although far from Oscar worthy, ain't all that bad (with the exception of Ginger Lynn Allen, who we know can do marvellous 'French', but whose Irish is lousy).<br /><br />Evil Breed opens in superb style with the brutal slaughter of a couple of amorous campers: after some brief under-canvas sex, the silicone enhanced hottie is dragged from the tent and torn in half; the guy has his arms and legs cut off and is roasted on a spit. It's a very gory start, and bodes well for the rest of the film.<br /><br />Unfortunately, after this promising beginning, things start to go seriously downhill: we are introduced to the main characters, an annoying bunch of twenty-somethings just begging to become cannibal chow, and are subjected to a fair amount of time wasting in the form of some terrible false scares, a lot of blarney about murderous druids from local Irish weirdo Gary (Simon Peacock), and worst of all, some sub-Scream, post-modernistic conversation about the conventions of horror films (how clever!).<br /><br />Then, just as it looks as though the film is never going to get any better, director Christian Viel decides to get serious: a guy gets a knife rammed through his head and there's a gratuitous sex-in-the-shower scene featuring lovely blonde Gillian Leigh (NOT a porn star, but I'm sure there's a career there waiting if she wants it). After that, things improve rapidly as the cannibals kick into top flesh-eating gear, and the film is transformed into a veritable bloodbath: Gary has a machete rammed up his ass (about time!), and is strangled with his intestines; Ginger Lynn kick-boxes a mutant; Jenna Jameson is torn open, eviscerated and has her silicone breast implant gnawed on by confused cannibal; a guy gets decapitated by cheese wire; and Taylor Hayes is seen bloody, bruised and naked with a dead foetus between her legs (apparantly, she's been captured and used as breeding stock).<br /><br />All of this is so outrageously gory that it makes sitting through the less interesting stuff worthwhile, and earns Evil Breed a final rating of 7/10.<br /><br />NB. A very troubled production and studio meddling resulted in Christian Viel eventually abandoning the project. Re-shoots were done and the gore was heavily trimmed for a US release. The good news is that although the film doesn't flow as well as it might have, and is cursed with a terrible ending, the UK DVD (the version I watched) seems to have been left relatively intact as far as the splatter is concerned (only 13s were cut from the film in total).\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 0\n",
      "Actual 1\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/pos/9311_10.txt\n",
      "This was the worst movie I've ever seen, yet it was also the best movie. Sci Fi original movie's are supposed to be bad, that's what makes them fun! The line, \"I like my dinosaur meat well done!\" is probably the best quote ever! Also, the plot sounds like something out of a pot induced dream. I can imagine it now, the writers waking up after a long night of getting high and playing dance dance revolution, then putting ideas together for this: Space marines got to alien planet, which is infested with dinosaurs and has medieval houses in it, to protect a science team studying the planet. Best idea ever! In fact, in fits the complete Sci Fi original movie checklist: guns dinosaurs medieval times space travel terrible acting<br /><br />So go watch this movie, but don't buy it.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 1\n",
      "Actual 0\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/neg/1404_3.txt\n",
      "Arthur Askey's great skill as a comic was in the way he communicated with his public. His juvenile jokes, silly songs and daft dances went down well because he was able to engage folk and draw them into his off the wall world. A lack of a live audience was a distinct disadvantage to him, and he was never completely comfortable in films. He has his moments in The Ghost Train, and his character, Tommy Gander, has been tailored to make the most of his talents, but Askey the performer needed to be seen to be appreciated.<br /><br />Askey's support in the film is not strong, it includes regular co-star Richard Murdoch; Betty Jardine and Stuart Latham as a dopey honeymoon couple; Linden Travers going over the top as a 'mad woman'. Also on board are Peter Murray-Hill, who off-screen married Phyllis Calvert, as the nominal leading man, giving a totally bland reading of the part, and leading lady Carol Lynne, who turns in an equally insipid performance. It is left to character actress Kathleen Harrison to effortlessly steal the film as a parrot loving single woman who gets smashed on Dr Morland Graham's brandy.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "predicted 1\n",
      "Actual 0\n",
      "/home/cvh255/nlp_hw1/aclImdb/train/neg/9391_2.txt\n",
      "My kids picked this out at the video store...it's great to hear Liza as Dorothy cause she sounds just like her mom. But there are too many bad songs, and the animation is pretty crude compared to other cartoons of that time.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(fns)):\n",
    "    print(\"predicted\",pred_false[i][0])\n",
    "    print(\"Actual\",actual_out[i])\n",
    "    print(val_df[val_df['file_names'] ==fns[i]][\"file_names\"].values[0])\n",
    "    f = open(val_df[val_df['file_names'] ==fns[i]][\"file_names\"].values[0])\n",
    "    print(f.read())\n",
    "    print()\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
